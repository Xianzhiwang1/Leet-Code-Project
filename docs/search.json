[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Final Project on prediction of Incorporation using replication data file on Russian Historical Factory level data from Professor Gregg, Middlebury College, Vermont\n\n\n\n\n\n\nMay 2, 2023\n\n\nPrateek and Xianzhi\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/final-blog-post/svm_test.html",
    "href": "posts/final-blog-post/svm_test.html",
    "title": "Our CSCI 0451 Final Project",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\n\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nFP = FinalProject()\ndf = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n/tmp/ipykernel_179926/4231091402.py:7: DtypeWarning: Columns (3,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n\n\ndf['form_dummy'] = np.where(df['Form']==1, 1, -1)\n\n\ndf.drop([\"Form\"], axis=1, inplace=True)\n\n\ndf = df.rename(columns={\"form_dummy\":\"Form\"})\n\n\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      id\n      PSZ\n      PSZ1900\n      FoundingYear\n      Province\n      Region\n      Industry\n      OntheSide\n      Age\n      ...\n      Wool\n      Cotton\n      MixedMaterials\n      Wood\n      Paper\n      MetalsandMachines\n      Foods\n      Chemical\n      Mineral\n      Form\n    \n  \n  \n    \n      0\n      0\n      4156\n      NaN\n      NaN\n      1860.0\n      30\n      CentralIndustrial\n      Paper\n      0.0\n      35.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      -1\n    \n    \n      1\n      1\n      52010\n      NaN\n      NaN\n      NaN\n      51\n      Previslitskii\n      Foods A\n      NaN\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      -1\n    \n    \n      2\n      2\n      30937\n      NaN\n      NaN\n      NaN\n      41\n      CentralBlacksoil\n      Foods A\n      NaN\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      -1\n    \n  \n\n3 rows × 66 columns\n\n\n\n\n\ndf = FP.balanced_one_minus_one(df)\ntrain, validate, test = FP.split_data(df)\ndf_train, X_train, y_train = FP.prepare_data(train)\ndf_validate, X_validate, y_validate= FP.prepare_data(validate)\ndf_test, X_test, y_test = FP.prepare_data(test)\n\ndf incorporated have 2393 many rows\nafter balancing, df not incorporated have 2393 many rows\n\n\n\ndf = df[['id', 'Form', 'FoundingYear',\n       'Province', 'Region', 'Industry', 'OntheSide', 'Age', 'TaxedActivity',\n       'YEAR', 'SubindustryCode', 'STCAP', 'Revenue',\n       'TotalWorkers', 'TotalPower', 'GrandTotalWorkers', 'RevperWorker',\n       'PowerperWorker', 'RevperGrandWorker', 'PowerperGrandWorker',\n       'logRevperWorker', 'logPowerperWorker', 'logRevperGrandWorker',\n       'logPowerperGrandWorker', 'logRev', 'logWorkers', 'logPower',\n       'RegIndGroup', 'RegIndYearGroup', 'ProvIndGroup', 'ProvIndYearGroup',\n       'IndYearGroup', 'IndustryFactor', 'ProvinceFactor', 'YearFactor',\n       'AKTS', 'PAI', 'factory_id', 'FormNextYear', 'FormNextNextYear',\n       'FactoryisCorpin1894', 'FormNextYearin1894', 'FactoryisCorpin1900',\n       'FormNextYearin1900', 'FactoryisCorpin1908', 'NEWDEV', 'SHARES',\n       'STPRICE', 'BONDS', 'Silk', 'Flax', 'Animal', 'Wool', 'Cotton',\n       'MixedMaterials', 'Wood', 'Paper', 'MetalsandMachines', 'Foods',\n       'Chemical', 'Mineral']]\n\n\ncols = ['PowerperWorker', 'RevperGrandWorker']\ncols = ['logWorkers', 'logPower'] \nNR = Newton_Raphson()\nFP = FinalProject()\nX_train,y_train = FP.make_ready_for_regression(X_train, y_train, cols) \nX_validate,y_validate= FP.make_ready_for_regression(X_validate, y_validate, cols) \nX_test, y_test = FP.make_ready_for_regression(X_test, y_test, cols)\n\n\nmySVC = SVC(kernel=\"linear\", gamma=\"auto\", shrinking=False)\nmySVC.fit(X_test, y_test)\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\nfrom newton_raphson import Newton_Raphson \nNR2 = Newton_Raphson()\ny_train = y_train.reshape(-1)\ny_test = y_test.reshape(-1)\nNR2.simple_plot(model = mySVC, X = X_test, y = y_test, size_1=5, size_2=5)\nprint(\"SVC\")\n\nSVC\n\n\n\n\n\n\nprint(mySVC.coef_)\nprint(mySVC.intercept_)\n\n[[0.83995613 0.08197002]]\n[-3.79364818]\n\n\n\nrow_length = X_train.shape[1]+1\nprint(row_length)\n\n3\n\n\n\nmysvm = supp_vec_machine(row_length=row_length)\nmysvm.fit(X_test,y_test, max_iter=1e3, alpha=1, tol=1e-5, lamb=0.5)\n\niter_count: 10\niter_count: 20\niter_count: 30\niter_count: 40\niter_count: 50\niter_count: 60\niter_count: 70\niter_count: 80\niter_count: 90\niter_count: 100\niter_count: 110\niter_count: 120\niter_count: 130\niter_count: 140\niter_count: 150\niter_count: 160\niter_count: 170\niter_count: 180\niter_count: 190\niter_count: 200\niter_count: 210\niter_count: 220\niter_count: 230\niter_count: 240\niter_count: 250\niter_count: 260\niter_count: 270\niter_count: 280\niter_count: 290\niter_count: 300\niter_count: 310\niter_count: 320\niter_count: 330\niter_count: 340\niter_count: 350\niter_count: 360\niter_count: 370\niter_count: 380\niter_count: 390\nConverged with 400 iterations\nThe weights we end up with is: [[ 0.18285496]\n [ 0.03940883]\n [-0.33444781]]\n\n\n\nprint(mysvm.weights)\nprint(mysvm.beta)\n\n[[ 0.18285496]\n [ 0.03940883]\n [-0.33444781]]\n[[ 0.18285496]\n [ 0.03940883]\n [-0.33444781]]\n\n\n\nmysvm.big_plot(X_train, y_train, X_validate, y_validate, X_test, y_test, 9,3)\n\n\n\n\n\nfrom sklearn.datasets import make_moons\n\nX,y= make_moons(100, shuffle=True, noise = 0.2)\nplt.rcParams[\"figure.figsize\"] = (4,4)\nplt.scatter(X[:,0], X[:,1], c=y)\nlabels=plt.gca().set(xlabel=\"Feature 1\", ylabel=\"Feature 2\")\n\n\n\n\n\nmysvm1 = supp_vec_machine()\nmysvm1.fit(X,y, max_iter=1e3, alpha=1, tol=1e-6, lamb=0.5)\n\niter_count: 10\niter_count: 20\niter_count: 30\niter_count: 40\niter_count: 50\niter_count: 60\niter_count: 70\niter_count: 80\niter_count: 90\niter_count: 100\niter_count: 110\niter_count: 120\niter_count: 130\niter_count: 140\niter_count: 150\niter_count: 160\niter_count: 170\niter_count: 180\niter_count: 190\niter_count: 200\niter_count: 210\niter_count: 220\niter_count: 230\niter_count: 240\niter_count: 250\niter_count: 260\niter_count: 270\niter_count: 280\niter_count: 290\niter_count: 300\niter_count: 310\niter_count: 320\niter_count: 330\niter_count: 340\niter_count: 350\niter_count: 360\niter_count: 370\niter_count: 380\niter_count: 390\niter_count: 400\niter_count: 410\niter_count: 420\niter_count: 430\niter_count: 440\niter_count: 450\niter_count: 460\niter_count: 470\niter_count: 480\niter_count: 490\niter_count: 500\niter_count: 510\niter_count: 520\niter_count: 530\niter_count: 540\niter_count: 550\niter_count: 560\niter_count: 570\niter_count: 580\niter_count: 590\niter_count: 600\niter_count: 610\niter_count: 620\niter_count: 630\niter_count: 640\niter_count: 650\niter_count: 660\niter_count: 670\niter_count: 680\niter_count: 690\niter_count: 700\niter_count: 710\niter_count: 720\niter_count: 730\niter_count: 740\niter_count: 750\niter_count: 760\niter_count: 770\niter_count: 780\niter_count: 790\niter_count: 800\niter_count: 810\niter_count: 820\niter_count: 830\niter_count: 840\niter_count: 850\niter_count: 860\niter_count: 870\niter_count: 880\niter_count: 890\niter_count: 900\niter_count: 910\niter_count: 920\niter_count: 930\niter_count: 940\niter_count: 950\niter_count: 960\niter_count: 970\niter_count: 980\niter_count: 990\n\n\n\nyourSVC = SVC(kernel=\"linear\", gamma=\"auto\", shrinking=False)\nyourSVC.fit(X, y)\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\nprint(yourSVC.coef_)\nprint(yourSVC.intercept_)\n\n[[ 0.62099012 -1.89131465]]\n[0.16174384]"
  },
  {
    "objectID": "posts/final-blog-post/test.html",
    "href": "posts/final-blog-post/test.html",
    "title": "Our CSCI 0451 Final Project",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nfrom final_project_code import FinalProject \nfrom newton_raphson import Newton_Raphson\nfrom final_plot import plot_stuff\n\n\n# from sklearn.metrics.pairwise import rbf_kernel\n\nFP = FinalProject()\ndf = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n/tmp/ipykernel_13695/3545232409.py:3: DtypeWarning: Columns (3,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n\n\ndf = FP.create_balanced_data(df)\n# df = df.sample(n=10000, replace=False)\ntrain, validate, test = FP.split_data(df)\n\ndf incorporated have 2393 many rows\nafter balancing, df not incorporated have 2393 many rows\n\n\n\ncols=['TotalWorkers','TotalPower']\n# RegionCoded is not generated here\ncols = [\"RegionCoded\", \"FactoryisCorpin1900\", \"NEWDEV\"]\ncols = cols[1:]\n\n\ndf_train, X_train, y_train = FP.prepare_data(train)\ndf_validate, X_validate, y_validate= FP.prepare_data(validate)\ndf_test, X_test, y_test = FP.prepare_data(test)\n\n\ncols = ['PowerperWorker', 'RevperGrandWorker']\ncols = ['logWorkers', 'logPower'] \n\n\n# only run once, convert to numpy\nX_train, y_train = FP.make_ready_for_regression(X_train, y_train, cols)\nX_validate, y_validate= FP.make_ready_for_regression(X_validate, y_validate, cols)\nX_test, y_test= FP.make_ready_for_regression(X_test, y_test, cols)\n\n\nfrom newton_raphson import Newton_Raphson\nNR = Newton_Raphson() \nNR.regress(y = y_train, X = X_train, max_iters = 1e3, tol=1e-15, converged=False)\n\nlearning rate is: 0.5\nRegularization is: True\nnumber of iteration: 10\nbeta: [[  516.28805794]\n [ 1787.67195079]\n [-1429.11939363]]\n\n\n/home/xianzhiwang/ml0451/ml-0451-final-proj/posts/final-blog-post/newton_raphson.py:35: RuntimeWarning: overflow encountered in exp\n  return 1/(1+np.exp(-x))\n\n\nnumber of iteration: 20\nbeta: [[  263.27490383]\n [   96.02404839]\n [-1355.20349548]]\nnumber of iteration: 30\nbeta: [[ 179.27917401]\n [   7.14642413]\n [-787.34272352]]\nnumber of iteration: 40\nbeta: [[  54.02617268]\n [   2.11965938]\n [-237.16886663]]\nnumber of iteration: 50\nbeta: [[ 0.73015557]\n [ 0.11186911]\n [-3.29529669]]\nnumber of iteration: 60\nbeta: [[ 1.06334261]\n [ 0.10968743]\n [-4.65452511]]\nnumber of iteration: 70\nbeta: [[ 1.06405021]\n [ 0.10969094]\n [-4.65747183]]\nnumber of iteration: 80\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nnumber of iteration: 90\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nnumber of iteration: 100\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nConverged with 104 iterations\nThe beta we end up with is: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\n\n\n\nNR.big_plot(X_train, y_train, X_validate, y_validate, X_test, y_test, 30, 10)\n\n\n\n\n\nNR.bare_bone_plot(X_train, y_train, 5,5)\nprint(NR.score(X_train,y_train))\nprint(NR.score(X_test,y_test))\nprint(NR.score(X_validate,y_validate))\n\nthe weight beta is: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\n0.8025078369905956\n0.8173277661795407\n0.7899686520376176\n\n\n\n\n\n\n# from final_plot import plot_stuff\n# PS = plot_stuff()\ncols = ['logWorkers', 'logPower'] \n\n\nLR = LogisticRegression(solver=\"newton-cg\")\nLR.fit(X_train, y_train)\nLR.coef_\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\narray([[1.06218722, 0.10987587]])\n\n\n\nLRscoreTest = LR.score(X_test, y_test)\nprint(LRscoreTest)\nLRscoreTrain= LR.score(X_train, y_train)\nprint(LRscoreTrain)\n\n0.8173277661795407\n0.8025078369905956\n\n\n\ny_test= y_test.reshape(-1)\nNR.simple_plot(LR, X_test, y_test, 5, 5)\n\n\n\n\n\nprint(LR.coef_)\na_0 = LR.coef_[0][0]\na_1 = LR.coef_[0][1]\nprint(LR.intercept_[0])\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfig = plt.scatter(X_train[:,0], X_train[:,1], c = y_train)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nf1 = np.linspace(3.5,4.5, 501)\np = plt.plot(f1, - (LR.intercept_/a_1) - (a_0/a_1)*f1, color = \"black\")\ntitle = plt.gca().set_title(f\"score= {round(LRscoreTrain,3)} using {LR}\")\n\n[[1.06218722 0.10987587]]\n-4.650112405682134\n\n\n\n\n\n\nfrom newton_raphson import Newton_Raphson\nNR2 = Newton_Raphson()\n\n\nDTree = DecisionTreeClassifier(random_state=0)\nDTree.fit(X_train, y_train)\nDTree.score(X_train, y_train)\n\n0.9191919191919192\n\n\n\ny_train = y_train.reshape(-1)\nNR2.simple_plot(DTree, X_train, y_train, 4,4)\n\n\n\n\n\nRandomForest = RandomForestClassifier()\nRandomForest.fit(X_train, y_train)\nRandomForest.score(X_train, y_train)\n\n0.9191919191919192\n\n\n\nNR2.simple_plot(RandomForest, X_train, y_train, 4,4)\n\n\n\n\n\nSVC2 = SVC(kernel=\"linear\", gamma = \"auto\", shrinking=False)\n\n\nSVC2.fit(X_train, y_train)\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\nNR2.simple_plot(SVC2, X_train, y_train,5,5)"
  },
  {
    "objectID": "posts/final-blog-post/index.html",
    "href": "posts/final-blog-post/index.html",
    "title": "ML Final Project",
    "section": "",
    "text": "The source code for this project could be found here. We have a README.md file that outlines what our project want to achieve, and roughly how we are going to implement the models and analysis. We also have a separate .txt file that gives a dictionary of all the variable names and their actual meaning. Hence, we encourage the reader to also reference that .txt file to remember which variable means what."
  },
  {
    "objectID": "posts/final-blog-post/index.html#some-historical-context-about-russia-and-descriptions-about-the-russians-system-of-incorporation",
    "href": "posts/final-blog-post/index.html#some-historical-context-about-russia-and-descriptions-about-the-russians-system-of-incorporation",
    "title": "ML Final Project",
    "section": "Some historical context about Russia and descriptions about the Russians system of incorporation",
    "text": "Some historical context about Russia and descriptions about the Russians system of incorporation\nSince the development of corporations in the 16th century, the legal codes allowing them to exist have been adopted by nearly all countries. Most countries, rightly, treated the idea of corporations with both excitement and apprehension. This is certainly true for Imperial Russia in the late 19th century. Russia observed the rapid advancement of Western European states such as England, Holland and Germany as a security threat. It wanted to create a system, with its limited resources, that allows for the existence corporations, while also maintaining a high degree of regulation.\nHence emerged the “Concession System of Incorporation” in Russia. Under this system, incorporation was granted in a slow, tedious and expensive manner on a case-by-case basis. The high costs and logistical challenges posed a serious barrier towards firms wishing to incorporate. Hence, under these circumstances, only some firms would benefit from incorporation, and would choose to incorporate. In this project, we wish to study the characteristics of firms that would choose to incorporate in this context. Doing so can allow us to better understand the economy of early 20th century Russia, the adoption and development of corporations, and the fundamental nature of corporations.\nOur work verifies and builds upon Professor Amanda Gregg’s work on corporations in Russia in “Factory Productivity and the Concession System of Incorporation in Late Imperial Russia, 1894–1908”"
  },
  {
    "objectID": "posts/final-blog-post/index.html#data",
    "href": "posts/final-blog-post/index.html#data",
    "title": "ML Final Project",
    "section": "Data",
    "text": "Data\nWe get our data from the Imperial Russian Factory Database, 1894-1908, compiled by Professor Gregg. This dataset is a digitzation of the Russian factory censuses from 1894, 1900 and 1908. This data is at the factory level, and includes information about the size of the factory’s workforce, the total power of its equipment, its revenue, and whether the firm owning it incorporated or not. It also contains information about the location of the firm and the date of its incorporation.\nWe hope to use this data to examine the types of industries that would benefit most from incorporation.\n\ninsert viz+statistics about number of factories belonging to corporations and non-corporations\ninsert visualization for number of factories by sector and ownership\ninsert visualization for factories by workforce, split by sector and by corporations/ non-corporations\ninsert visualization for factories by power used, splot by sector and corporations/ non-corporations\ninsert same by revenue\n\nFirst, let us import some libraries that will become useful down the road. Also, the following snippet will automatically reload the final_project_code.py file where we keep our functions.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom final_project_code import FinalProject \nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nRead in the data\nLet us create an instance of the class we defined in final_project_code.py, and let us read in the entire data set.\n\n# from sklearn.metrics.pairwise import rbf_kernel\nFP = FinalProject()\ndf = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\", low_memory=False)\nRvss = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\", low_memory=False)\n\nLet us now observe some summary statistics about our dataset.\nThe following visualization demonstrates the number of corporations and non-corporations in our dataset:\n\ncorporation_percentage = (df['Form'].value_counts(normalize=True) * 100).loc[1]\n\nsns.barplot(x=['Corporation', 'Non-corporation'], y=[corporation_percentage, 100 - corporation_percentage])\n\n<AxesSubplot:>\n\n\n\n\n\nAs we can observe, we have significantly more factories owned by non-corporations than we do factories owned by corporations. This has the potential to impact our research methods. We will discuss this further later in the blog post. Let us also break this chart down by the type of industry.\n\ndf['Ownership'] = df['Form'].map({1: 'Corporation', 0: 'Non-corporation'})\ng = sns.catplot(x='Industry', hue='Ownership', data=df, kind='count', height=8, aspect=1.5)\n\n\n\n\nWe can already initially observe that certain industries how a high proportion of factories owned by corporations compared to other industries. For example\nFirst, let’s do some data visualization. We are interested in seeing which industry in Late Imperial Russia had high machine power (measured in horsepower) and have high number of workers. We also want to get a sense of the distribution of machine power and number of workers, and visualize them by industry. Hence, let’s focus on the picture below. We see that roughly, factory with more machine power tend to also have more workers, and most company cluster at the \\(2000\\) horse power level, and \\(2500\\) workers.\n\nsns.jointplot(data=df, x=\"TotalWorkers\", y = \"TotalPower\", hue=\"Industry\")\n\n<seaborn.axisgrid.JointGrid at 0x7fd4d9be28e0>\n\n\n\n\n\nSimilarly, here’s another plot to visualize the unbalanced nature of the data set. Here, Form is the desired label that we want to predict. Form taking a value of \\(1\\) means that factory was incorporated, i.e., it was owned by a incorporated firm. If Form take the value of \\(0\\), then that factory was not incorporated. In the next plot, instead of TotalPower, which stands for Total amount of horse power and TotalWorkers, which stands for total number of workers, we use logPowerperWorker and logRevperWorker as our y-axis and our x-axis. logPowerperWorker is obtained by taking the log of \\(\\frac{Power}{Worker}\\), and logRevperWorker is log of \\(\\frac{Revenue}{Worker}\\). And the hue is whether the factory is encorporated or not. Again, we see that the orange dots, which corresponds to \\(1\\), which corresponds to encorporated, is a very small percentage of all the factories. Most factories are not encorporated. Also, we observe that the data points follows a bell-shaped distribution on the two dimensions.\n\nsns.jointplot(data=df, x=\"logRevperWorker\", y=\"logPowerperWorker\", hue = \"Form\")\n\n<seaborn.axisgrid.JointGrid at 0x7faf28213c40>"
  }
]