[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Final Project on prediction of Incorporation using replication data file on Russian Historical Factory level data from Professor Gregg, Middlebury College, Vermont\n\n\n\n\n\n\nApr 20, 2023\n\n\nPrateek and Xianzhi\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/final-blog-post/index.html",
    "href": "posts/final-blog-post/index.html",
    "title": "ML Final Project",
    "section": "",
    "text": "First, let us import some libraries that will become useful down the road. Also, the following snippet will automatically reload the final_project_code.py file where we keep our functions.\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom final_project_code import FinalProject \n\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\n\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nRead in the data\nLet us create an instance of the class we defined in final_project_code.py, and let us read in the entire data set.\n\nFP = FinalProject()\n\n\nRvss = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n/tmp/ipykernel_82390/139095848.py:1: DtypeWarning: Columns (3,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  Rvss = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n\nFirst, let’s do some data visualization. We are interested in seeing which industry in Late Imperial Russia had high machine power (measured in horsepower) and have high number of workers. We also want to get a sense of the distribution of machine power and number of workers, and visualize them by industry. Hence, let’s focus on the picture below. We see that roughly, factory with more machine power tend to also have more workers, and most company cluster at the \\(2000\\) horse power level, and \\(2500\\) workers.\n\nsns.jointplot(data=Rvss, x=\"TotalWorkers\", y = \"TotalPower\", hue=\"Industry\")\n\n<seaborn.axisgrid.JointGrid at 0x7f22877389a0>\n\n\n\n\n\nSimilarly, here’s another plot to visualize the unbalanced nature of the data set. Here, Form is the desired label that we want to predict. Form taking a value of \\(1\\) means that factory was incorporated, i.e., it was owned by a incorporated firm. If Form take the value of \\(0\\), then that factory was not incorporated. In the next plot, instead of TotalPower, which stands for Total amount of horse power and TotalWorkers, which stands for total number of workers, we use logPowerperWorker and logRevperWorker as our y-axis and our x-axis. logPowerperWorker is obtained by taking the log of \\(\\frac{Power}{Worker}\\), and logRevperWorker is log of \\(\\frac{Revenue}{Worker}\\). And the hue is whether the factory is encorporated or not. Again, we see that the orange dots, which corresponds to \\(1\\), which corresponds to encorporated, is a very small percentage of all the factories. Most factories are not encorporated. Also, we observe that the data points follows a bell-shaped distribution on the two dimensions.\n\nsns.jointplot(data=Rvss, x=\"logRevperWorker\", y=\"logPowerperWorker\", hue = \"Form\")\n\n<seaborn.axisgrid.JointGrid at 0x7f2286d1da00>\n\n\n\n\n\n\n\nPredicting whether Russian Factories want to incorporate or not\n\n\nFirst approach, rebalance the data set by random sampling\nWe start our analysis by using logistic regression to predict what kind of firms in late Imperial Russia is more likely owned by a corporation. Since our data is not balanced, we use several different approach to this problem and try them one at a time. Luckily for us, classification problem with unbalanced data labels is quite common, so we have many approaches at our disposal.\nLet us first get out definitions straight. Unbalanced data refers to those datasets where the target label has an uneven distribution of observations, First, we try to randomly sample the majority data set, which in this case, is when the label equals unincorporated. Then we keep all the data entries of the minority data set, and add in the randomly sampled extract of the majority data set with size equal to the minority data set. Then we perform logistic regression on this new data set. The good news is that our new data set is balanced, and the bad news is that we loose a lot of information by discarding many data entries in the majority data set.\nWhether to incorporate or not is an interesting question for factories and firms in late Imperial Russia. There were many factors that might affect a firm’s decision to incorporate or not, including the overall size of the factory, which could be seen in features such as total machine power of the factory and the total number of workers in a factory. Other factors such as the geographical location of the factory (i.e. which region it was located in) could also play a role. Since the decision to incorporate could be affected by many features, we see that some feature engineering could be beneficial for our analysis.\nWe download the replication data set and put it in the same directory as our project. After we read in the data, we notice that there are \\(66\\) columns, which means potentially we could have around \\(60\\) features for our machine learning model. However, let’s start small. Hence, we begin our analysis using a subset of the columns. Also, since in the original data set, there’s only a small percentage of factories that are incorporated, which is because of historical reasons in Late Imperial Russia during 1894 to 1908. For the purpose of this machine learning project, we artificially select a subset of the whole data set so that we have equal number of factories owned by incorporated firms and not incorporated firms alike.\n\nRvss.loc[Rvss['Form']==0]\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      id\n      Form\n      PSZ\n      PSZ1900\n      FoundingYear\n      Province\n      Region\n      Industry\n      OntheSide\n      ...\n      Animal\n      Wool\n      Cotton\n      MixedMaterials\n      Wood\n      Paper\n      MetalsandMachines\n      Foods\n      Chemical\n      Mineral\n    \n  \n  \n    \n      0\n      0\n      4156\n      0\n      NaN\n      NaN\n      1860.0\n      30\n      CentralIndustrial\n      Paper\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      1\n      52010\n      0\n      NaN\n      NaN\n      NaN\n      51\n      Previslitskii\n      Foods A\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      2\n      2\n      30937\n      0\n      NaN\n      NaN\n      NaN\n      41\n      CentralBlacksoil\n      Foods A\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      3\n      3\n      39923\n      0\n      NaN\n      NaN\n      NaN\n      3\n      Caucasus\n      Metals and Machines\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      4\n      3296\n      0\n      NaN\n      NaN\n      1882.0\n      11\n      Northwestern\n      Chemicals\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      37964\n      37964\n      34420\n      0\n      NaN\n      NaN\n      NaN\n      30\n      CentralIndustrial\n      Mineral Products\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      37965\n      37965\n      34210\n      0\n      NaN\n      NaN\n      NaN\n      19\n      CentralIndustrial\n      Mineral Products\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      37966\n      37966\n      52603\n      0\n      NaN\n      NaN\n      NaN\n      17\n      Southern\n      Foods A\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      37967\n      37967\n      4886\n      0\n      NaN\n      NaN\n      NaN\n      61\n      CentralIndustrial\n      Mineral Products\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      37968\n      37968\n      37189\n      0\n      NaN\n      NaN\n      NaN\n      8\n      Caucasus\n      Silk\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n37895 rows × 66 columns\n\n\n\n\ndf = Rvss[['id', 'Form', 'FoundingYear',\n       'Province', 'Region', 'Industry', 'OntheSide', 'Age', 'TaxedActivity',\n       'YEAR', 'SubindustryCode', 'STCAP', 'Revenue',\n       'TotalWorkers', 'TotalPower', 'GrandTotalWorkers', 'RevperWorker',\n       'PowerperWorker', 'RevperGrandWorker', 'PowerperGrandWorker',\n       'logRevperWorker', 'logPowerperWorker', 'logRevperGrandWorker',\n       'logPowerperGrandWorker', 'logRev', 'logWorkers', 'logPower',\n       'RegIndGroup', 'RegIndYearGroup', 'ProvIndGroup', 'ProvIndYearGroup',\n       'IndYearGroup', 'IndustryFactor', 'ProvinceFactor', 'YearFactor',\n       'AKTS', 'PAI', 'factory_id', 'FormNextYear', 'FormNextNextYear',\n       'FactoryisCorpin1894', 'FormNextYearin1894', 'FactoryisCorpin1900',\n       'FormNextYearin1900', 'FactoryisCorpin1908', 'NEWDEV', 'SHARES',\n       'STPRICE', 'BONDS', 'Silk', 'Flax', 'Animal', 'Wool', 'Cotton',\n       'MixedMaterials', 'Wood', 'Paper', 'MetalsandMachines', 'Foods',\n       'Chemical', 'Mineral']]\n\n\nresult = FP.create_balanced_data(df)\n\ndf incorporated have 2393 many rows\nafter balancing, df not incorporated have 2393 many rows\n\n\n\nresult['Form'].mean()\n\n0.5\n\n\n\ncompression_opts = dict(method='zip', archive_name='artificially_balanced_Rvssian_Factory.csv')\nresult.to_csv('artificially_balanced_Rvssian_Factory.zip', index = False, compression=compression_opts)\n\n\nFP = FinalProject()\ntrain, validate, test = FP.split_data(result)\n\n\ndf_train, X_train, y_train = FP.prepare_data(train)\ndf_validate, X_validate, y_validate= FP.prepare_data(validate)\ndf_test, X_test, y_test = FP.prepare_data(test)\n\n\n\nLet’s do some visualization\n\nsns.lmplot(data=df_train, x=\"TotalWorkers\", y = \"TotalPower\", hue=\"Form\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f2279d60640>\n\n\n\n\n\n\nsns.lmplot(data=df_train, x=\"PowerperWorker\", y = \"RevperWorker\", hue=\"Form\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f2279d40cd0>\n\n\n\n\n\n\n\nFirst, we try some feature engineering\nIn the basic toolbox of an economist, one is unlikely to find methods of feature engineering, since we believe that economists would rather choose the regressors (features) themselves, becuase regressors are often central to the economic questoin and analysis. However, in this project, we actually going to have a systematic way written in code to select the features, according to which combination of features gives the highest score.\n\nX_train, RegionCoded =   FP.encode_features(X_train, \"Region\")\nX_train['RegionCoded'] = RegionCoded\n\n\nX_train, IndustryCoded = FP.encode_features(X_train, \"Industry\")\nX_train[\"IndustryCoded\"] = IndustryCoded\nX_train = X_train.fillna(0)\n# X_train.head(10)\n\n\nX_train, IndustryFactorCoded = FP.encode_features(X_train, 'IndustryFactor')\nX_train[\"IndustryFactorCoded\"] = IndustryFactorCoded\n\nIndex(['id', 'FoundingYear', 'Province', 'OntheSide', 'Age', 'TaxedActivity',\n       'YEAR', 'SubindustryCode', 'STCAP', 'Revenue', 'TotalWorkers',\n       'TotalPower', 'GrandTotalWorkers', 'RevperWorker', 'PowerperWorker',\n       'RevperGrandWorker', 'PowerperGrandWorker', 'logRevperWorker',\n       'logPowerperWorker', 'logRevperGrandWorker', 'logPowerperGrandWorker',\n       'logRev', 'logWorkers', 'logPower', 'RegIndGroup', 'RegIndYearGroup',\n       'ProvIndGroup', 'ProvIndYearGroup', 'IndYearGroup', 'ProvinceFactor',\n       'YearFactor', 'AKTS', 'PAI', 'factory_id', 'FormNextYear',\n       'FormNextNextYear', 'FactoryisCorpin1894', 'FormNextYearin1894',\n       'FactoryisCorpin1900', 'FormNextYearin1900', 'FactoryisCorpin1908',\n       'NEWDEV', 'SHARES', 'STPRICE', 'BONDS', 'Silk', 'Flax', 'Animal',\n       'Wool', 'Cotton', 'MixedMaterials', 'Wood', 'Paper',\n       'MetalsandMachines', 'Foods', 'Chemical', 'Mineral', 'RegionCoded',\n       'IndustryCoded', 'IndustryFactorCoded'],\n      dtype='object')\n\n\n\nFP1 = FinalProject() \nall_qual_cols = ['RegionCoded', 'IndustryCoded', \"IndustryFactorCoded\"] \n\nall_quant_cols = [\n       'Province', 'OntheSide', 'Age', 'TaxedActivity',\n       'YEAR', 'SubindustryCode', 'STCAP', 'Revenue',\n       'TotalWorkers', 'TotalPower', 'GrandTotalWorkers', 'RevperWorker',\n       'PowerperWorker', 'RevperGrandWorker', 'PowerperGrandWorker',\n       'logRevperWorker', 'logPowerperWorker', 'logRevperGrandWorker',\n       'logPowerperGrandWorker', 'logRev', 'logWorkers', 'logPower',\n       'RegIndGroup', 'RegIndYearGroup', 'ProvIndGroup', 'ProvIndYearGroup',\n       'IndYearGroup', 'ProvinceFactor', 'YearFactor',\n       'AKTS', 'PAI', 'factory_id', 'FormNextYear', 'FormNextNextYear',\n       'FactoryisCorpin1894', 'FormNextYearin1894', 'FactoryisCorpin1900',\n       'FormNextYearin1900', 'FactoryisCorpin1908', 'NEWDEV', 'SHARES',\n       'STPRICE', 'BONDS', 'Silk', 'Flax', 'Animal', 'Wool', 'Cotton',\n       'MixedMaterials', 'Wood', 'Paper', 'MetalsandMachines', 'Foods',\n       'Chemical', 'Mineral']\ny_train = y_train.reset_index(drop=True)\nFP1.feature_combo(all_qual_cols, all_quant_cols, X_train, y_train)    \n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nmax(FP1.feature_score_pair, key=FP1.feature_score_pair.get)\n\n('RegionCoded', 'FactoryisCorpin1900', 'NEWDEV')\n\n\nHence, for fitting logistic regression on the training data, the highest scoring combination of 3 features among the independent variables that we considered is RegionCoded, RevperGrandWorker, logWorkers.\n\n# test for max \nmax([-10, 5, 3], key=abs)\nprint(max(FP1.feature_score_pair.values()))\nprint(FP1.feature_score_pair[('RegionCoded', 'RevperGrandWorker', 'logWorkers')])\nprint(FP1.feature_score_pair[(\"RegionCoded\", \"FactoryisCorpin1900\", \"NEWDEV\")])\n\n0.9944270289097875\n0.8025078369905956\n0.9944270289097875\n\n\n\nThe Prediction Question:\nCan we predict which factory belongs to a incorporated company in Late Imerial Russia, during year 1894 and year 1908, by looking at other variables that are in the data?\n\n\nData Inspection\nSince previously we have artificially selected a subset of our entire Russian factory data set so that we have equal number of factories belonging to incorporated firms and not incorporated firms, we expect that our label has roughly \\(50 \\%\\) of \\(1\\)’s and \\(50 \\%\\) % of \\(0\\)’s.\n\ny_train.mean()\n\n0.5019157088122606\n\n\n\ndf_train.groupby(['Industry'])[['Form']].aggregate([np.mean, len]).round(3)\n\n\n\n\n\n  \n    \n      \n      Form\n    \n    \n      \n      mean\n      len\n    \n    \n      Industry\n      \n      \n    \n  \n  \n    \n      Animal\n      0.205\n      190\n    \n    \n      Chemicals\n      0.737\n      190\n    \n    \n      Cotton\n      0.827\n      346\n    \n    \n      Flax\n      0.663\n      92\n    \n    \n      Foods A\n      0.274\n      482\n    \n    \n      Metals and Machines\n      0.627\n      515\n    \n    \n      Mineral Products\n      0.429\n      280\n    \n    \n      Mixed Materials\n      0.354\n      65\n    \n    \n      Paper\n      0.505\n      218\n    \n    \n      Silk\n      0.354\n      48\n    \n    \n      Wood\n      0.414\n      232\n    \n    \n      Wool\n      0.441\n      213\n    \n  \n\n\n\n\nHence, it seems that Food industry in Late Imperial Russia had a low incorporation rate, which is around \\(28.3 \\%\\). On the other hand, the Cotton industry had a relatively high incorporation rate, around \\(81.4 \\%\\).\n\ndf_train.groupby(['Industry'])[['TotalPower']].aggregate([np.mean, len]).round(3)\n\n\n\n\n\n  \n    \n      \n      TotalPower\n    \n    \n      \n      mean\n      len\n    \n    \n      Industry\n      \n      \n    \n  \n  \n    \n      Animal\n      41.061\n      190\n    \n    \n      Chemicals\n      280.530\n      190\n    \n    \n      Cotton\n      1283.686\n      346\n    \n    \n      Flax\n      534.179\n      92\n    \n    \n      Foods A\n      72.457\n      482\n    \n    \n      Metals and Machines\n      255.735\n      515\n    \n    \n      Mineral Products\n      57.301\n      280\n    \n    \n      Mixed Materials\n      78.791\n      65\n    \n    \n      Paper\n      165.861\n      218\n    \n    \n      Silk\n      41.906\n      48\n    \n    \n      Wood\n      66.531\n      232\n    \n    \n      Wool\n      194.150\n      213\n    \n  \n\n\n\n\nWe see that Cotton industry has the highest mean total power, and Silk industry has the lowest mean total power. We might predict that industry with a higher need for capital might choose to incorporate. ### After doing feature engineering, we perform a standard regression analysis using the combination of features with the highest scores. ### Running some logistic regressions and plotting ROC curves We start with the standard procedure in Econometrics, which is running regressions. We first perform some regression analysis that is close in spirit to the published paper where this replication data set is coming from, and use ROC curve and confusion matrix to assess our model performance. First, let’s get our definition straight.\n\nTrue Positive Rate = True Positives / (True Positives + False Negatives)\nFalse Positive Rate = False Positives / (False Positives + True Negatives)\n\nThe ROC curve is especially useful when we want to compare directly the curve of several different models. Also, AUC, which stands for the area under the curve can be used to measure how good a model is.\n\n\nimportant_cols = ['RegionCoded', 'RevperGrandWorker', 'logWorkers']\nhigh_scoring_cols = [\"RegionCoded\", \"FactoryisCorpin1900\", \"NEWDEV\"]\n# X_train = X_train.dropna()\n# X_train = X_train.drop([\"Industry\"], axis=1)\n# X_train = X_train.drop([\"Region\"], axis=1)\n# X_train.drop([\"FoundingYear\", \"OntheSide\", \"TaxedActivity\", \"PSZLastYear\", \"PSZ1908\"], axis=1)\nX_train = X_train.fillna(0)\ny_train.reset_index(drop=True)\n\n# cols = [\"FactoryisCorpin1900\", \"NEWDEV\"]\ncols = ['RevperGrandWorker', 'logWorkers']\n# fit a model\nLR = LogisticRegression(solver=\"newton-cg\") # Newton's Method\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.8021595262974573\n\n\n\nX_test = X_test.fillna(0)\n\n\n# ROC curve, receiver operating characteristic\n# TPR true positive rate\n# FPR false positive rate\n\n# predict the probability\nprobability = LR.predict_proba(X_test[cols])\n\n\n# get the probabilities for the positive outcome\nproba_positive = probability[:,1]\n\n\n# get a no-skill prediction that always predict the majority class\nproba_no_skill = [0 for _ in range(len(y_test))]\n\n\n# compute the scores\nauc_no_skill = roc_auc_score(y_test, proba_no_skill) \nauc_LR = roc_auc_score(y_test, proba_positive)\nprint(f\"Always predict zero, which is not incorporate. ROC AUC = {auc_no_skill}\")\nprint(f\"Use Logistic Regression. ROC AUC = {auc_LR}\")\n# fpr, tpr, thresholds = roc_curve(y_train, scores, pos_label=2)\n\nAlways predict zero, which is not incorporate. ROC AUC = 0.5\nUse Logistic Regression. ROC AUC = 0.8655317145883183\n\n\n\nprint(confusion_matrix(y_test, LR.predict(X_test[cols]), normalize=\"true\"))\nFP.print_confusion_matrix(LR, X_test[cols], y_test)\n\n[[0.78197065 0.21802935]\n [0.22037422 0.77962578]]\n\n\n\n\n\n\n# compute roc curves\nfpr_no_skill, tpr_no_skil, _ = roc_curve(y_test, proba_no_skill)\nfpr_LR, tpr_LR, _ = roc_curve(y_test, proba_positive)\n# plot the roc curve for the model\nplt.plot(fpr_no_skill, tpr_no_skil, linestyle=\"--\", label=\"No Skill\")\nplt.plot(fpr_LR, tpr_LR, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive rate')\nplt.ylabel('True Positive rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()\n\n\n\n\n\nWe are interested in seeing the coefficients for the regressors in our logistic regression. We see that the coefficient for revenue per worker grand total RevperGrandWorker is \\(0.000056\\), and the coefficient for logWorkers, the log of the number of workers is \\(1.048\\). In the cells below, we are also interested in visualizing the decision region of our model. We see that the desion boundary is linear, and since our data are all relatively clustered together, the picture does not really show that our model is doing a fantastic job. It is quite hard to guess how many data points are in the correct region because of it’s quite dense.\n\npd.DataFrame({\n    \"column\": X_train[cols].columns,\n    \"coefficient\": LR.coef_.ravel()\n})\n\n\n\n\n\n  \n    \n      \n      column\n      coefficient\n    \n  \n  \n    \n      0\n      RevperGrandWorker\n      0.000056\n    \n    \n      1\n      logWorkers\n      1.048007\n    \n  \n\n\n\n\n\ntype(X_train[cols])\ncol=['TotalWorkers','TotalPower']\nX_test[col].head(50)\n\n\n\n\n\n  \n    \n      \n      TotalWorkers\n      TotalPower\n    \n  \n  \n    \n      126\n      318.0\n      0.0\n    \n    \n      3950\n      1931.0\n      0.0\n    \n    \n      2935\n      88.0\n      5.0\n    \n    \n      3204\n      167.0\n      9.0\n    \n    \n      1347\n      8.0\n      18.0\n    \n    \n      3055\n      45.0\n      40.0\n    \n    \n      355\n      16.0\n      0.0\n    \n    \n      4462\n      189.0\n      0.0\n    \n    \n      3531\n      5402.0\n      0.0\n    \n    \n      2730\n      3751.0\n      1215.0\n    \n    \n      827\n      1.0\n      10.0\n    \n    \n      1639\n      777.0\n      187.0\n    \n    \n      3694\n      1433.0\n      1331.0\n    \n    \n      3040\n      1214.0\n      520.0\n    \n    \n      984\n      10.0\n      4.0\n    \n    \n      3281\n      8.0\n      39.0\n    \n    \n      4758\n      6.0\n      0.0\n    \n    \n      3930\n      148.0\n      6.0\n    \n    \n      2188\n      2169.0\n      0.0\n    \n    \n      4420\n      668.0\n      180.0\n    \n    \n      2443\n      15.0\n      0.0\n    \n    \n      362\n      3170.0\n      1560.0\n    \n    \n      2349\n      63.0\n      4.0\n    \n    \n      345\n      256.0\n      200.0\n    \n    \n      3887\n      53.0\n      125.0\n    \n    \n      4416\n      4.0\n      0.0\n    \n    \n      711\n      4.0\n      0.0\n    \n    \n      3205\n      64.0\n      5.0\n    \n    \n      3908\n      55.0\n      10.0\n    \n    \n      4591\n      30.0\n      2.0\n    \n    \n      3893\n      25.0\n      15.0\n    \n    \n      3325\n      16.0\n      0.0\n    \n    \n      10\n      152.0\n      16.0\n    \n    \n      1683\n      173.0\n      80.0\n    \n    \n      2116\n      44.0\n      28.0\n    \n    \n      4536\n      164.0\n      22.0\n    \n    \n      1834\n      628.0\n      475.0\n    \n    \n      793\n      11.0\n      0.0\n    \n    \n      3363\n      605.0\n      1000.0\n    \n    \n      4733\n      13.0\n      0.0\n    \n    \n      2802\n      171.0\n      0.0\n    \n    \n      2674\n      80.0\n      670.0\n    \n    \n      94\n      2746.0\n      646.0\n    \n    \n      3903\n      202.0\n      1.0\n    \n    \n      129\n      366.0\n      28.0\n    \n    \n      3756\n      8.0\n      0.0\n    \n    \n      2891\n      11935.0\n      7240.0\n    \n    \n      2371\n      175.0\n      203.0\n    \n    \n      2094\n      1950.0\n      0.0\n    \n    \n      3880\n      7.0\n      108.0\n    \n  \n\n\n\n\n\nprint(isinstance(X_train[cols], np.ndarray))\n# need to write up the function that make a visual representation\nvalue=1.5\nwidth=0.75\nplot_decision_regions(X_test[cols].to_numpy(), y_test.to_numpy(), clf=LR\n                    #   filler_feature_values={2:value},\n                    #   filler_feature_ranges={2:width}\n                      )\nmypredict = LR.predict(X_test[cols].to_numpy())\ntitle = plt.gca().set(title=f\"Accuracy={(mypredict==y_test).mean()}\",\n                      xlabel=\"Feature 1\",\n                      ylabel=\"Feature 2\")\n\nFalse\n\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\nNow we try Polynomial Features\n\n\nUse cross validation\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\n\nNow, we are unsure if using polynomial features would give our model more predictive power. One way to find out is by trying different degrees of polynomials and score them using cross validation. The idea behind cross validation is as follows: we divide the data into little chunks, and in each regression, one of the chunks is used as validation data set, and all other chunks are used as training data set. Each little chunk takes turn to be used for validation, hence the name cross validation. We could take the average of all those scores and use this average to compare models with different degrees.\n\ndef poly_LR(deg):\n    return Pipeline([(\"poly\", PolynomialFeatures(degree=deg)),\n                     (\"LR\", LogisticRegression(penalty=\"none\", max_iter=int(1e3)))])\n\nplr = poly_LR(deg = 2)\n\n\ncv_scores = cross_val_score(plr, X_train[cols], y_train, cv=5)\ncv_scores\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\narray([0.5026087 , 0.50174216, 0.50174216, 0.50522648, 0.50522648])\n\n\n\ncv_scores.mean()\n\n0.5033091955764278\n\n\nHence, this is telling us that for the features we selected, polynomial logistic regression has roughly the same predictive power as simply guessing whether a factory is belonging to a corporation or not. Degree zero corresponds to the baseline model, and degree 1 corresponds to simple logistic regression without a polynomial feature map.\nIn the above code snippets, we defined a model using degree 2 polynomial features, did cross validation by dividing data into 5 chunks, and then we take the mean to get the final score. Now, we put this into a function with a for loop, that will lop through each\n\nFP.polynomial_degree_validation(X_train[cols], y_train, 4, 5)\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nPolynomial degree = 0, score = 0.502\nPolynomial degree = 1, score = 0.803\nPolynomial degree = 2, score = 0.503\nPolynomial degree = 3, score = 0.493\n\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nHence, we see that degree one has the highest score, meaning that we just need simple logistic regression in this case. Since this score comes from cross validation, we could rely on the accuracy of this score to some extent. Now, we use simple logistic regression with no polynomial features to fit our training data once again, and we test on the testing data.\n\nplr = poly_LR(1)\nplr.fit(X_train[cols], y_train)\nplr.score(X_train[cols], y_train)\nX_test = X_test.fillna(0)\nplr.score(X_test[cols], y_test).round(4)\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\n0.7808\n\n\nWe get a score of \\(0.78\\), which is not bad! Also, let us print out the classification report for our model. Again, our precision score is not bad!\n\ny_predict = plr.predict(X_test[cols])\nprint(classification_report(y_test, y_predict))\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.78      0.78       477\n           1       0.78      0.78      0.78       481\n\n    accuracy                           0.78       958\n   macro avg       0.78      0.78      0.78       958\nweighted avg       0.78      0.78      0.78       958\n\n\n\n\nprint(confusion_matrix(y_test, y_predict))\nprint(confusion_matrix(y_test, y_predict, normalize = \"true\"))\n\n[[373 104]\n [106 375]]\n[[0.78197065 0.21802935]\n [0.22037422 0.77962578]]\n\n\nRecall that the confusion matrix is organized as follows, so we could read the data right off the matrix. \\[\n\\begin{matrix}\n\\text{True Positive} & \\text{False Positive} \\\\\n\\text{False Negative} & \\text{True Negative} \\\\\n\\end{matrix}\n\\] In our case, we have \\(373\\) true positives and \\(375\\) true negatives for our test data, and \\(104\\) false positives and \\(106\\) false negatives. This is quite evenly distributed, our model does not give more false positives than false negatives or the other way round. Hence, our model’s prediction is not biased towards positive or negative. Also, by specifying normalize = \"true\", we could get the True Positive Rate (TPR), False Positive Rate (FPR), and so on. Feel free to check the definition given above earlier.\nNow, let’s find out the mean machine power per worker for our testing set X_test, which is \\(0.6991\\). Now, we would like to filter our test data X_test and only keep the entries that have more than average power per worker. Then we print out our compusion matrix again just for those entries with more than average machine power per worker. We are interested to see if our model is biased or not, in the sense that it might give more False Positives than False Negatives, or the other way round.\n\nX_test[\"PowerperWorker\"].mean()\n\n0.699119351282881\n\n\n\nix = X_test[\"PowerperWorker\"] > 0.63483127\nprint(\"Factories with more power per worker than average\")\nprint(f\"The percentage our prediction is correct: {(y_test[ix] == y_predict[ix]).mean()}\")\nconfusion_matrix(y_test[ix], y_predict[ix])\n\nFactories with more power per worker than average\nThe percentage our prediction is correct: 0.8144796380090498\n\n\narray([[82, 17],\n       [24, 98]])\n\n\nWe see that our model has predicted false negative for \\(24\\) cases, and false positive for \\(17\\) cases for factories with more power per worker than average. Hence, in this case, our model is slightly biased towards false negative, predicting the factory is not incorporated (negative) when the factory is actually incorporated (positive). This makes economic sense, since we have selected only factories with above average machine power, and since factories with more machine power stood to gain more investment and capital if they incorporate, they were more likely to incorporate than average, so our model, which is trained on all the entries in X_train, has predicted more false negatives for factories with more power, and this fits our economic intuition.\n\n\nix = X_test[\"PowerperWorker\"] < 0.63483127\nprint(\"Factories with less power per worker than average\")\nprint(f\"The percentage our prediction matches the actual label: {(y_test[ix] == y_predict[ix]).mean()}\")\nprint(confusion_matrix(y_test[ix], y_predict[ix]))\nprint(confusion_matrix(y_test[ix], y_predict[ix], normalize=\"true\"))\n\nFactories with less power per worker than average\nThe percentage our prediction matches the actual label: 0.7706919945725916\n[[291  87]\n [ 82 277]]\n[[0.76984127 0.23015873]\n [0.22841226 0.77158774]]\n\n\nWe do the exact same thing for restricting our data set to only factories with less machine power per worker than average. The situation is exactly flipped, since we have more false positive than false negative. Our model tend to predict incorporated when the factory was not incorporated. This is consistent with our findings earlier, since we are in the exact flip case, where we restrict to factories with less machine power, and those are factories that potentially benefit less from the access to more captial and credit brought by incorporation, since they are not perticularly machine intensive. Hence, they had a slightly lower probability to incorporate."
  }
]