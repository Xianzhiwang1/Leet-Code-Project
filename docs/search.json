[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "“I thought you quit?” “but I started again this blog on Dec 17, 2023.” I was going to start during summer 2023, but did not get to it. We’ll get to the vacation later."
  },
  {
    "objectID": "posts/blog-05-len-of-last-word/index.html",
    "href": "posts/blog-05-len-of-last-word/index.html",
    "title": "(Leet Code Problem 58) Length of Last Word and More Problems",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nSome Leet Code problems\nThe source code of our implementation could be found in this directory on github along with this blog post.\n\nfrom main import *\n\n\n# problem 58 length of last word\nmystring = \"this is a big dawg   \"\nprint(f\"inverted string: {mystring[::-1]}\")\nprint(f\"length of entire string: {len(mystring)}\")\nmysoln = Solution()\nprint(f\"length of last word: {mysoln.lengthOfLastWord(mystring)}\")\n\ninverted string:    gwad gib a si siht\nlength of entire string: 21\nlength of last word: 4\n\n\n\n# problem 9 Palindrome Number\nmysoln2 = Solution()\nmyint = 115696511\nprint(mysoln2.isPalindrome(myint))\nmyint2 = 123923842\nprint(mysoln2.isPalindrome(myint2))\n\n# using the second implementation\nprint(mysoln2.isPalindromeII(myint))\nprint(mysoln2.isPalindromeII(myint2))\nmyint3 = 141\nmyint4 = 5665\nprint(mysoln2.isPalindromeII(myint3))\nprint(mysoln2.isPalindromeII(myint4))\n\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\n\n\n\n# longest common prefix\nstrs = [\"dog\", \"dogpoop\", \"dogfood\",\"dogday\"]\nstrs2 = [\"bigdawg\", \"lildawg\", \"crib\", \"whip\", \"stack\"]\nprint(f\"longest common prefix is: {mysoln2.longestCommonPrefix(strs)}\")\nprint(f\"longest common prefix is: {mysoln2.longestCommonPrefix(strs2)}\")\n\nlongest common prefix is: dog\nlongest common prefix is: \n\n\n\n# generate an image \nfrom matplotlib import pyplot as plt\nimport numpy as np\n# np.random.random_integers(low,high,shape)\nM = np.random.randint(0,256, (5,7,3))\n\n\n# we just want an image for our blog entry\nplt.imshow(M, interpolation='none')\n\n&lt;matplotlib.image.AxesImage at 0x7f3306adfa50&gt;"
  },
  {
    "objectID": "posts/blog-02-test/test.html",
    "href": "posts/blog-02-test/test.html",
    "title": "Test",
    "section": "",
    "text": "Test LogisticRegression\n%load_ext autoreload %autoreload 2\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nfrom final_project_code import FinalProject \nfrom newton_raphson import Newton_Raphson\nfrom final_plot import plot_stuff\n\n\n# from sklearn.metrics.pairwise import rbf_kernel\n\nFP = FinalProject()\ndf = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n/tmp/ipykernel_13695/3545232409.py:3: DtypeWarning: Columns (3,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n\n\ndf = FP.create_balanced_data(df)\n# df = df.sample(n=10000, replace=False)\ntrain, validate, test = FP.split_data(df)\n\ndf incorporated have 2393 many rows\nafter balancing, df not incorporated have 2393 many rows\n\n\n\ncols=['TotalWorkers','TotalPower']\n# RegionCoded is not generated here\ncols = [\"RegionCoded\", \"FactoryisCorpin1900\", \"NEWDEV\"]\ncols = cols[1:]\n\n\ndf_train, X_train, y_train = FP.prepare_data(train)\ndf_validate, X_validate, y_validate= FP.prepare_data(validate)\ndf_test, X_test, y_test = FP.prepare_data(test)\n\n\ncols = ['PowerperWorker', 'RevperGrandWorker']\ncols = ['logWorkers', 'logPower'] \n\n\n# only run once, convert to numpy\nX_train, y_train = FP.make_ready_for_regression(X_train, y_train, cols)\nX_validate, y_validate= FP.make_ready_for_regression(X_validate, y_validate, cols)\nX_test, y_test= FP.make_ready_for_regression(X_test, y_test, cols)\n\n\nfrom newton_raphson import Newton_Raphson\nNR = Newton_Raphson() \nNR.regress(y = y_train, X = X_train, max_iters = 1e3, tol=1e-15, converged=False)\n\nlearning rate is: 0.5\nRegularization is: True\nnumber of iteration: 10\nbeta: [[  516.28805794]\n [ 1787.67195079]\n [-1429.11939363]]\nnumber of iteration: 20\nbeta: [[  263.27490383]\n [   96.02404839]\n [-1355.20349548]]\nnumber of iteration: 30\nbeta: [[ 179.27917401]\n [   7.14642413]\n [-787.34272352]]\nnumber of iteration: 40\nbeta: [[  54.02617268]\n [   2.11965938]\n [-237.16886663]]\nnumber of iteration: 50\nbeta: [[ 0.73015557]\n [ 0.11186911]\n [-3.29529669]]\nnumber of iteration: 60\nbeta: [[ 1.06334261]\n [ 0.10968743]\n [-4.65452511]]\nnumber of iteration: 70\nbeta: [[ 1.06405021]\n [ 0.10969094]\n [-4.65747183]]\nnumber of iteration: 80\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nnumber of iteration: 90\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nnumber of iteration: 100\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nConverged with 104 iterations\nThe beta we end up with is: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\n\n\n/home/xianzhiwang/ml0451/ml-0451-final-proj/posts/final-blog-post/newton_raphson.py:35: RuntimeWarning: overflow encountered in exp\n  return 1/(1+np.exp(-x))\n\n\n\nNR.big_plot(X_train, y_train, X_validate, y_validate, X_test, y_test, 30, 10)\n\n\n\n\n\nNR.bare_bone_plot(X_train, y_train, 5,5)\nprint(NR.score(X_train,y_train))\nprint(NR.score(X_test,y_test))\nprint(NR.score(X_validate,y_validate))\n\nthe weight beta is: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\n0.8025078369905956\n0.8173277661795407\n0.7899686520376176\n\n\n\n\n\n\n# from final_plot import plot_stuff\n# PS = plot_stuff()\ncols = ['logWorkers', 'logPower'] \n\n\nLR = LogisticRegression(solver=\"newton-cg\")\nLR.fit(X_train, y_train)\nLR.coef_\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\narray([[1.06218722, 0.10987587]])\n\n\n\nLRscoreTest = LR.score(X_test, y_test)\nprint(LRscoreTest)\nLRscoreTrain= LR.score(X_train, y_train)\nprint(LRscoreTrain)\n\n0.8173277661795407\n0.8025078369905956\n\n\n\ny_test= y_test.reshape(-1)\nNR.simple_plot(LR, X_test, y_test, 5, 5)\n\n\n\n\n\nprint(LR.coef_)\na_0 = LR.coef_[0][0]\na_1 = LR.coef_[0][1]\nprint(LR.intercept_[0])\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfig = plt.scatter(X_train[:,0], X_train[:,1], c = y_train)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nf1 = np.linspace(3.5,4.5, 501)\np = plt.plot(f1, - (LR.intercept_/a_1) - (a_0/a_1)*f1, color = \"black\")\ntitle = plt.gca().set_title(f\"score= {round(LRscoreTrain,3)} using {LR}\")\n\n[[1.06218722 0.10987587]]\n-4.650112405682134\n\n\n\n\n\n\nfrom newton_raphson import Newton_Raphson\nNR2 = Newton_Raphson()\n\n\nDTree = DecisionTreeClassifier(random_state=0)\nDTree.fit(X_train, y_train)\nDTree.score(X_train, y_train)\n\n0.9191919191919192\n\n\n\ny_train = y_train.reshape(-1)\nNR2.simple_plot(DTree, X_train, y_train, 4,4)\n\n\n\n\n\nRandomForest = RandomForestClassifier()\nRandomForest.fit(X_train, y_train)\nRandomForest.score(X_train, y_train)\n\n0.9191919191919192\n\n\n\nNR2.simple_plot(RandomForest, X_train, y_train, 4,4)\n\n\n\n\n\nSVC2 = SVC(kernel=\"linear\", gamma = \"auto\", shrinking=False)\n\n\nSVC2.fit(X_train, y_train)\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\nNR2.simple_plot(SVC2, X_train, y_train,5,5)"
  },
  {
    "objectID": "posts/blog-04-add-binary/index.html",
    "href": "posts/blog-04-add-binary/index.html",
    "title": "(Leet Code Problem) Add Binary Solution in Python",
    "section": "",
    "text": "We need to take care of adding two digits together in binary, which is done mod 2, and we also need to implement the “carry” term.\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom source import *\n\n\nsoln_object = Solution()\nanswer = soln_object.addBinary(\"1010\", \"1011\")\nprint(answer)\n\nanswer = soln_object.addBinary(\"11\", \"1\")\nprint(answer)\n\nanswer = soln_object.addBinary(\"11111\", \"1\")\nprint(answer)\n\n\n\n10101\n100\n100000\n\n\nWe need to sum up the \\(2^0\\) th degit of the first number with the \\(2^0\\) th degit of the second number, then sum up the \\(2^1\\) th degit, and also have a carry term.\nWhen we sum together \\(1\\) and \\(1\\) in binary, we get \\(0\\) and carry = 1, so we need to do arithmetic mod \\(2\\) . Then we use floor divide by \\(2\\) to determine the carry term.\n\n# use mod 2 to compute the digit\nx = 1 % 2\nprint(x)\n\n1\n\n\n\n# use floor divide for the carry term\nx = 1 // 2\ny = 3 // 2\nz = 2 // 2\nprint(x)\nprint(y)\nprint(z)\n\n0\n1\n1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "December’s Very Own blog",
    "section": "",
    "text": "(Leet Code Problem 58) Length of Last Word and More Problems\n\n\n\n\n\nsolution to Leet Code problem 58 and more.\n\n\n\n\n\n\nJan 14, 2024\n\n\nShawnZ\n\n\n\n\n\n\n  \n\n\n\n\n(Leet Code Problem) Add Binary Solution in Python\n\n\n\n\n\nsolution to Leet Code problem Add Binary.\n\n\n\n\n\n\nDec 17, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\nML Final Project\n\n\n\n\n\nFinal Project on prediction of Incorporation using replication data file on Russian Historical Factory level data from Professor Gregg, Middlebury College, Vermont\n\n\n\n\n\n\nMay 2, 2023\n\n\nPrateek and Xianzhi\n\n\n\n\n\n\n  \n\n\n\n\nCompare Newton Raphson and Logistic Regression\n\n\n\n\n\nA Test of the Newton Raphson we implemented\n\n\n\n\n\n\nApr 10, 2023\n\n\nXianzhi\n\n\n\n\n\n\n  \n\n\n\n\nSVM\n\n\n\n\n\nLearn SVM\n\n\n\n\n\n\nApr 5, 2023\n\n\nXianzhi\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog-03-ml-0451-proj/index.html",
    "href": "posts/blog-03-ml-0451-proj/index.html",
    "title": "ML Final Project",
    "section": "",
    "text": "We aim to study the history and effects of incorporation in Russia. Our project uses historical primary data from Russian factory censuses which were digitized, cleand and curated by Professor Amanda Gregg and are freely available on the American Economic Association’s website. While economic historians frequently bicker about the origins and effectiveness of corporations- most agree that the 19th century witnessed the rise of the corporate business form in Europe and North America. Firms in Russia began incorporatinng relatively late- in the late 19th century. We aim to study the factors that may lead to a firm choosing to incorporate- and to study the effects of incorporation on the firm itself. We intend to use supervised learning and feature engineering to study the variables that are the most influenced incorporation, and the variables that make incorporation more likely. We need to note that this is an unbalanced data set, in the sense that most factories are not incorporated.\nWe have used this replication data set to test our machine learning models (logistic regression, etc) and make predictions. We looked out for bias in machine learning, since our data set is unbalanced. Also, it is a historical fact that there was only a low percentage of incorporation going on in Late Imperial Russia at that time. Our data comes from (here)[https://www.aeaweb.org/articles?id=10.1257/aer.20151656], and our question is how to predict whether a factory incorporate or not in that period. Our model should be about to give predictions that are more or less consistent with the arguments presented in the published paper, “Factory Productivity and the Concession System of Incorporation in Late Imperial Russia, 1894-1908.” We focused on using supervised learning to look at how different features relate to the dependent variable, which is incorporated or not. Our main result is using logistic regression to predict if factories would incorporate or not, and our model is quite useful and agrees with our economics intuition.\nThe source code for this project could be found here. We have a README.md file that outlines what our project want to achieve, and roughly how we are going to implement the models and analysis. We also have a separate .txt file that gives a dictionary of all the variable names and their actual meaning. Hence, we encourage the reader to also reference that .txt file to remember what does each variable represent."
  },
  {
    "objectID": "posts/blog-03-ml-0451-proj/index.html#some-historical-context-about-russia-and-descriptions-about-the-russians-system-of-incorporation",
    "href": "posts/blog-03-ml-0451-proj/index.html#some-historical-context-about-russia-and-descriptions-about-the-russians-system-of-incorporation",
    "title": "ML Final Project",
    "section": "Some historical context about Russia and descriptions about the Russians system of incorporation",
    "text": "Some historical context about Russia and descriptions about the Russians system of incorporation\nSince the development of corporations in the 16th century, the legal codes allowing them to exist have been adopted by nearly all countries. Most countries, rightly, treated the idea of corporations with both excitement and apprehension. This is certainly true for Imperial Russia in the late 19th century. Russia observed the rapid advancement of Western European states such as England, Holland and Germany as a security threat. It wanted to create a system, with its limited resources, that allows for the existence corporations, while also maintaining a high degree of regulation.\nHence emerged the “Concession System of Incorporation” in Russia. Under this system, incorporation was granted in a slow, tedious and expensive manner on a case-by-case basis. The high costs and logistical challenges posed a serious barrier towards firms wishing to incorporate. Hence, under these circumstances, only some firms would benefit from incorporation, and would choose to incorporate. In this project, we wish to study the characteristics of firms that would choose to incorporate in this context. Doing so can allow us to better understand the economy of early 20th century Russia, the adoption and development of corporations, and the fundamental nature of corporations.\nOur work verifies and builds upon Professor Amanda Gregg’s work on corporations in Russia in “Factory Productivity and the Concession System of Incorporation in Late Imperial Russia, 1894–1908”"
  },
  {
    "objectID": "posts/blog-03-ml-0451-proj/index.html#data",
    "href": "posts/blog-03-ml-0451-proj/index.html#data",
    "title": "ML Final Project",
    "section": "Data",
    "text": "Data\nWe get our data from the Imperial Russian Factory Database, 1894-1908, compiled by Professor Gregg. This dataset is a digitzation of the Russian factory censuses from 1894, 1900 and 1908. This data is at the factory level, and includes information about the size of the factory’s workforce, the total power of its equipment, its revenue, and whether the firm owning it incorporated or not. It also contains information about the location of the firm and the date of its incorporation.\nWe hope to use this data to examine the types of industries that would benefit most from incorporation.\n\ninsert viz+statistics about number of factories belonging to corporations and non-corporations\ninsert visualization for number of factories by sector and ownership\ninsert visualization for factories by workforce, split by sector and by corporations/ non-corporations\ninsert visualization for factories by power used, splot by sector and corporations/ non-corporations\ninsert same by revenue\n\nFirst, let us import some libraries that will become useful down the road. Also, the following snippet will automatically reload the final_project_code.py file where we keep our functions.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom final_project_code import FinalProject \nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nRead in the data\nLet us create an instance of the class we defined in final_project_code.py, and let us read in the entire data set.\n\n# from sklearn.metrics.pairwise import rbf_kernel\nFP = FinalProject()\ndf = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\", low_memory=False)\nRvss = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\", low_memory=False)\n\nLet us now observe some summary statistics about our dataset.\nThe following visualization demonstrates the number of corporations and non-corporations in our dataset:\n\ncorporation_percentage = (df['Form'].value_counts(normalize=True) * 100).loc[1]\n\nsns.barplot(x=['Corporation', 'Non-corporation'], y=[corporation_percentage, 100 - corporation_percentage])\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nAs we can observe, we have significantly more factories owned by non-corporations than we do factories owned by corporations. This has the potential to impact our research methods. We will discuss this further later in the blog post. Let us also break this chart down by the type of industry.\n\ndf['Ownership'] = df['Form'].map({1: 'Corporation', 0: 'Non-corporation'})\ng = sns.catplot(x='Industry', hue='Ownership', data=df, kind='count', height=8, aspect=1.5)\n\n\n\n\nWe can already initially observe that certain industries how a high proportion of factories owned by corporations compared to other industries. For example, the metals and machines, cotton and chemical industries have a significantly higher proportion of factories being owned by incorporated firms.\nWe can also take a look at the number factories in each industry in our dataset. We observe that the food and metals industries have the greatest number of factories, while we have relatively few factories for silk and flax.\nLet us now take a look at the number of workers employed at each factory.\n\nsns.displot(df[\"TotalWorkers\"], kde=False, bins=50)\n#sns.histplot(df['TotalWorkers'], kde=False, color='green', log_scale=('y'))\n\n\n\n\nWe can see that have a large number of factories with very few workers, and very few factories with a large number of workers.\nLet us now compare this distribution with the distribution for only corporation-owned factories:\n\nsns.displot(df[df['Form']==1]['TotalWorkers'], kde=False, bins=50, stat='density', common_norm=False)\nplt.ylabel('Proportion')\nplt.show()\n\n\n\n\nAs we can see, while observe a similar trend for corporation owned factories- this distribution has a fatter tail- indicating that factories owned by corporations have more workers. Let us perform the same analysis for total power per factory:\n\nsns.displot(df[\"TotalPower\"], kde=False, bins=50)\n\n\n\n\n\nsns.displot(df[df['Form']==1]['TotalPower'], kde=False, bins=50, stat='density', common_norm=False)\nplt.ylabel('Proportion')\nplt.show()\n\n\n\n\nWe can draw similar inferences from our distribution for power: factories owned by corporations are more likely to employ a greater total power value.\nWe are interested in seeing which industry in Late Imperial Russia had high machine power (measured in horsepower) and have high number of workers. We also want to get a sense of the distribution of machine power and number of workers, and visualize them by industry. Hence, let’s focus on the picture below. We see that roughly, factory with more machine power tend to also have more workers, and most company cluster at the \\(2000\\) horse power level, and \\(2500\\) workers.\n\nsns.jointplot(data=df, x=\"TotalWorkers\", y = \"TotalPower\", hue=\"Industry\")\n\n\n\n\nSimilarly, here’s another plot to visualize the unbalanced nature of the data set. Here, Form is the desired label that we want to predict. Form taking a value of \\(1\\) means that factory was incorporated, i.e., it was owned by a incorporated firm. If Form take the value of \\(0\\), then that factory was not incorporated. In the next plot, instead of TotalPower, which stands for Total amount of horse power and TotalWorkers, which stands for total number of workers, we use logPowerperWorker and logRevperWorker as our y-axis and our x-axis. logPowerperWorker is obtained by taking the log of \\(\\frac{Power}{Worker}\\), and logRevperWorker is log of \\(\\frac{Revenue}{Worker}\\). And the hue is whether the factory is encorporated or not. Again, we see that the orange dots, which corresponds to \\(1\\), which corresponds to encorporated, is a very small percentage of all the factories. Most factories are not encorporated. Also, we observe that the data points follows a bell-shaped distribution on the two dimensions.\n\nsns.jointplot(data=df, x=\"logRevperWorker\", y=\"logPowerperWorker\", hue = \"Form\")"
  },
  {
    "objectID": "posts/blog-00-example/index.html",
    "href": "posts/blog-00-example/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/blog-00-example/index.html#math",
    "href": "posts/blog-00-example/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/blog-01-svm/svm_test.html",
    "href": "posts/blog-01-svm/svm_test.html",
    "title": "SVM",
    "section": "",
    "text": "My SVM implementation\nThe source code of this implementation could be found in this directory on github along with this blog post.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n# from final_project_code import FinalProject \n# from newton_raphson import Newton_Raphson\n# from final_plot import plot_stuff\nfrom sklearn.datasets import make_moons\nfrom main import supp_vec_machine\n\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\n# import seaborn as sns\n# from mlxtend.plotting import plot_decision_regions\n\n\n# mySVC = SVC(kernel=\"linear\", gamma=\"auto\", shrinking=False)\n# mySVC.fit(X_test, y_test)\n\n\n# mysvm = supp_vec_machine(row_length=row_length)\n# mysvm.fit(X_test,y_test, max_iter=1e3, alpha=1, tol=1e-5, lamb=0.5)\n\n\n# print(mysvm.weights)\n# print(mysvm.beta)\n\n\n# mysvm.big_plot(X_train, y_train, X_validate, y_validate, X_test, y_test, 9,3)\n\n\nX,y= make_moons(100, shuffle=True, noise = 0.2)\nX_train, y_train = make_moons(100, shuffle=True, noise = 0.28)\nX_validate, y_validate= make_moons(100, shuffle=True, noise = 0.30)\nX_test, y_test= make_moons(100, shuffle=True, noise = 0.32)\nplt.rcParams[\"figure.figsize\"] = (4,4)\nplt.scatter(X[:,0], X[:,1], c=y)\nlabels=plt.gca().set(xlabel=\"Feature 1\", ylabel=\"Feature 2\")\n\n\n\n\n\nsoln = SVC(kernel=\"linear\", gamma=\"auto\", shrinking=False)\nsoln.fit(X_train, y_train)\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\nprint(soln.coef_)\nprint(soln.intercept_)\n\n[[ 0.69190546 -1.63813064]]\n[-0.03091258]\n\n\n\nprint(soln.coef_)\na_0 = soln.coef_[0][0]\na_1 = soln.coef_[0][1]\nprint(soln.intercept_[0])\nplt.rcParams[\"figure.figsize\"] = (6,6)\nfig = plt.scatter(X_train[:,0], X_train[:,1], c = y_train)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nf1 = np.linspace(-1.5,3.5, 501)\np = plt.plot(f1, - (soln.intercept_/a_1) - (a_0/a_1)*f1, color = \"black\")\ntitle = plt.gca().set_title(f\"SVM using {soln}\")\n\n[[ 0.69190546 -1.63813064]]\n-0.030912584333047263\n\n\n\n\n\n\nmysvm = supp_vec_machine()\nmysvm.fit(X_train, y_train, max_iter=1e5, alpha=1, tol=1e-6, lamb=0.5)\n\niter_count: 10\niter_count: 20\niter_count: 30\niter_count: 40\niter_count: 50\niter_count: 60\niter_count: 70\niter_count: 80\niter_count: 90\niter_count: 100\niter_count: 110\niter_count: 120\niter_count: 130\niter_count: 140\niter_count: 150\niter_count: 160\niter_count: 170\niter_count: 180\niter_count: 190\niter_count: 200\niter_count: 210\niter_count: 220\niter_count: 230\niter_count: 240\niter_count: 250\niter_count: 260\niter_count: 270\niter_count: 280\niter_count: 290\niter_count: 300\niter_count: 310\niter_count: 320\niter_count: 330\niter_count: 340\niter_count: 350\niter_count: 360\niter_count: 370\niter_count: 380\niter_count: 390\niter_count: 400\niter_count: 410\niter_count: 420\niter_count: 430\niter_count: 440\niter_count: 450\niter_count: 460\niter_count: 470\niter_count: 480\niter_count: 490\niter_count: 500\niter_count: 510\niter_count: 520\niter_count: 530\niter_count: 540\niter_count: 550\niter_count: 560\niter_count: 570\niter_count: 580\niter_count: 590\niter_count: 600\niter_count: 610\niter_count: 620\niter_count: 630\niter_count: 640\niter_count: 650\niter_count: 660\niter_count: 670\niter_count: 680\niter_count: 690\niter_count: 700\niter_count: 710\niter_count: 720\niter_count: 730\niter_count: 740\niter_count: 750\niter_count: 760\niter_count: 770\niter_count: 780\niter_count: 790\niter_count: 800\niter_count: 810\niter_count: 820\niter_count: 830\niter_count: 840\niter_count: 850\niter_count: 860\niter_count: 870\niter_count: 880\niter_count: 890\niter_count: 900\niter_count: 910\niter_count: 920\niter_count: 930\niter_count: 940\niter_count: 950\niter_count: 960\niter_count: 970\niter_count: 980\niter_count: 990\niter_count: 1000\niter_count: 1010\niter_count: 1020\niter_count: 1030\niter_count: 1040\niter_count: 1050\niter_count: 1060\niter_count: 1070\niter_count: 1080\niter_count: 1090\niter_count: 1100\niter_count: 1110\niter_count: 1120\niter_count: 1130\niter_count: 1140\niter_count: 1150\niter_count: 1160\niter_count: 1170\niter_count: 1180\niter_count: 1190\niter_count: 1200\niter_count: 1210\niter_count: 1220\niter_count: 1230\niter_count: 1240\niter_count: 1250\niter_count: 1260\niter_count: 1270\niter_count: 1280\niter_count: 1290\niter_count: 1300\niter_count: 1310\niter_count: 1320\niter_count: 1330\niter_count: 1340\niter_count: 1350\niter_count: 1360\niter_count: 1370\niter_count: 1380\niter_count: 1390\niter_count: 1400\niter_count: 1410\niter_count: 1420\niter_count: 1430\niter_count: 1440\niter_count: 1450\niter_count: 1460\niter_count: 1470\niter_count: 1480\niter_count: 1490\niter_count: 1500\niter_count: 1510\niter_count: 1520\niter_count: 1530\niter_count: 1540\niter_count: 1550\niter_count: 1560\niter_count: 1570\niter_count: 1580\niter_count: 1590\niter_count: 1600\niter_count: 1610\niter_count: 1620\niter_count: 1630\niter_count: 1640\niter_count: 1650\niter_count: 1660\niter_count: 1670\niter_count: 1680\niter_count: 1690\niter_count: 1700\niter_count: 1710\niter_count: 1720\niter_count: 1730\niter_count: 1740\niter_count: 1750\niter_count: 1760\niter_count: 1770\niter_count: 1780\niter_count: 1790\niter_count: 1800\niter_count: 1810\niter_count: 1820\niter_count: 1830\niter_count: 1840\niter_count: 1850\niter_count: 1860\niter_count: 1870\niter_count: 1880\niter_count: 1890\niter_count: 1900\niter_count: 1910\niter_count: 1920\niter_count: 1930\niter_count: 1940\niter_count: 1950\niter_count: 1960\niter_count: 1970\niter_count: 1980\niter_count: 1990\niter_count: 2000\niter_count: 2010\niter_count: 2020\niter_count: 2030\niter_count: 2040\niter_count: 2050\niter_count: 2060\niter_count: 2070\niter_count: 2080\niter_count: 2090\niter_count: 2100\niter_count: 2110\niter_count: 2120\niter_count: 2130\niter_count: 2140\niter_count: 2150\niter_count: 2160\niter_count: 2170\niter_count: 2180\niter_count: 2190\niter_count: 2200\niter_count: 2210\niter_count: 2220\niter_count: 2230\niter_count: 2240\niter_count: 2250\niter_count: 2260\niter_count: 2270\niter_count: 2280\niter_count: 2290\niter_count: 2300\niter_count: 2310\niter_count: 2320\niter_count: 2330\niter_count: 2340\niter_count: 2350\niter_count: 2360\niter_count: 2370\niter_count: 2380\niter_count: 2390\niter_count: 2400\niter_count: 2410\niter_count: 2420\niter_count: 2430\niter_count: 2440\niter_count: 2450\niter_count: 2460\niter_count: 2470\niter_count: 2480\niter_count: 2490\niter_count: 2500\niter_count: 2510\niter_count: 2520\niter_count: 2530\niter_count: 2540\niter_count: 2550\niter_count: 2560\niter_count: 2570\niter_count: 2580\niter_count: 2590\niter_count: 2600\niter_count: 2610\niter_count: 2620\niter_count: 2630\niter_count: 2640\niter_count: 2650\niter_count: 2660\niter_count: 2670\niter_count: 2680\niter_count: 2690\niter_count: 2700\niter_count: 2710\niter_count: 2720\niter_count: 2730\niter_count: 2740\niter_count: 2750\niter_count: 2760\niter_count: 2770\niter_count: 2780\niter_count: 2790\niter_count: 2800\niter_count: 2810\niter_count: 2820\niter_count: 2830\niter_count: 2840\niter_count: 2850\niter_count: 2860\niter_count: 2870\niter_count: 2880\niter_count: 2890\niter_count: 2900\niter_count: 2910\niter_count: 2920\niter_count: 2930\niter_count: 2940\niter_count: 2950\niter_count: 2960\niter_count: 2970\niter_count: 2980\niter_count: 2990\niter_count: 3000\niter_count: 3010\niter_count: 3020\niter_count: 3030\niter_count: 3040\niter_count: 3050\niter_count: 3060\niter_count: 3070\niter_count: 3080\niter_count: 3090\niter_count: 3100\niter_count: 3110\niter_count: 3120\niter_count: 3130\niter_count: 3140\niter_count: 3150\niter_count: 3160\niter_count: 3170\niter_count: 3180\niter_count: 3190\niter_count: 3200\niter_count: 3210\niter_count: 3220\niter_count: 3230\niter_count: 3240\niter_count: 3250\niter_count: 3260\niter_count: 3270\niter_count: 3280\niter_count: 3290\niter_count: 3300\niter_count: 3310\niter_count: 3320\niter_count: 3330\niter_count: 3340\niter_count: 3350\niter_count: 3360\niter_count: 3370\niter_count: 3380\niter_count: 3390\niter_count: 3400\niter_count: 3410\niter_count: 3420\niter_count: 3430\niter_count: 3440\niter_count: 3450\niter_count: 3460\niter_count: 3470\niter_count: 3480\niter_count: 3490\niter_count: 3500\niter_count: 3510\niter_count: 3520\niter_count: 3530\niter_count: 3540\niter_count: 3550\niter_count: 3560\niter_count: 3570\niter_count: 3580\niter_count: 3590\niter_count: 3600\niter_count: 3610\niter_count: 3620\niter_count: 3630\niter_count: 3640\niter_count: 3650\niter_count: 3660\niter_count: 3670\niter_count: 3680\niter_count: 3690\niter_count: 3700\niter_count: 3710\niter_count: 3720\niter_count: 3730\niter_count: 3740\niter_count: 3750\niter_count: 3760\niter_count: 3770\niter_count: 3780\niter_count: 3790\niter_count: 3800\niter_count: 3810\niter_count: 3820\niter_count: 3830\niter_count: 3840\niter_count: 3850\niter_count: 3860\niter_count: 3870\niter_count: 3880\niter_count: 3890\niter_count: 3900\niter_count: 3910\niter_count: 3920\niter_count: 3930\niter_count: 3940\niter_count: 3950\niter_count: 3960\niter_count: 3970\niter_count: 3980\niter_count: 3990\niter_count: 4000\niter_count: 4010\niter_count: 4020\niter_count: 4030\niter_count: 4040\niter_count: 4050\niter_count: 4060\niter_count: 4070\niter_count: 4080\niter_count: 4090\niter_count: 4100\niter_count: 4110\niter_count: 4120\niter_count: 4130\niter_count: 4140\niter_count: 4150\niter_count: 4160\niter_count: 4170\niter_count: 4180\niter_count: 4190\niter_count: 4200\niter_count: 4210\niter_count: 4220\niter_count: 4230\niter_count: 4240\niter_count: 4250\niter_count: 4260\niter_count: 4270\niter_count: 4280\niter_count: 4290\niter_count: 4300\niter_count: 4310\niter_count: 4320\niter_count: 4330\niter_count: 4340\niter_count: 4350\niter_count: 4360\niter_count: 4370\niter_count: 4380\niter_count: 4390\niter_count: 4400\niter_count: 4410\niter_count: 4420\niter_count: 4430\niter_count: 4440\niter_count: 4450\niter_count: 4460\niter_count: 4470\niter_count: 4480\niter_count: 4490\niter_count: 4500\niter_count: 4510\niter_count: 4520\niter_count: 4530\niter_count: 4540\niter_count: 4550\niter_count: 4560\niter_count: 4570\niter_count: 4580\niter_count: 4590\niter_count: 4600\niter_count: 4610\niter_count: 4620\niter_count: 4630\niter_count: 4640\niter_count: 4650\niter_count: 4660\niter_count: 4670\niter_count: 4680\niter_count: 4690\niter_count: 4700\niter_count: 4710\niter_count: 4720\niter_count: 4730\niter_count: 4740\niter_count: 4750\niter_count: 4760\niter_count: 4770\niter_count: 4780\niter_count: 4790\niter_count: 4800\niter_count: 4810\niter_count: 4820\niter_count: 4830\niter_count: 4840\niter_count: 4850\niter_count: 4860\niter_count: 4870\niter_count: 4880\niter_count: 4890\niter_count: 4900\niter_count: 4910\niter_count: 4920\niter_count: 4930\niter_count: 4940\niter_count: 4950\niter_count: 4960\niter_count: 4970\niter_count: 4980\niter_count: 4990\niter_count: 5000\niter_count: 5010\niter_count: 5020\niter_count: 5030\niter_count: 5040\niter_count: 5050\niter_count: 5060\niter_count: 5070\niter_count: 5080\niter_count: 5090\niter_count: 5100\niter_count: 5110\niter_count: 5120\niter_count: 5130\niter_count: 5140\niter_count: 5150\niter_count: 5160\niter_count: 5170\niter_count: 5180\niter_count: 5190\niter_count: 5200\niter_count: 5210\niter_count: 5220\niter_count: 5230\niter_count: 5240\niter_count: 5250\niter_count: 5260\niter_count: 5270\niter_count: 5280\niter_count: 5290\niter_count: 5300\niter_count: 5310\niter_count: 5320\niter_count: 5330\niter_count: 5340\niter_count: 5350\niter_count: 5360\niter_count: 5370\niter_count: 5380\niter_count: 5390\niter_count: 5400\niter_count: 5410\niter_count: 5420\niter_count: 5430\niter_count: 5440\niter_count: 5450\niter_count: 5460\niter_count: 5470\niter_count: 5480\niter_count: 5490\niter_count: 5500\niter_count: 5510\niter_count: 5520\niter_count: 5530\niter_count: 5540\niter_count: 5550\niter_count: 5560\niter_count: 5570\niter_count: 5580\niter_count: 5590\niter_count: 5600\niter_count: 5610\niter_count: 5620\niter_count: 5630\niter_count: 5640\niter_count: 5650\niter_count: 5660\niter_count: 5670\niter_count: 5680\niter_count: 5690\niter_count: 5700\niter_count: 5710\niter_count: 5720\niter_count: 5730\niter_count: 5740\niter_count: 5750\niter_count: 5760\niter_count: 5770\niter_count: 5780\niter_count: 5790\niter_count: 5800\niter_count: 5810\niter_count: 5820\niter_count: 5830\niter_count: 5840\niter_count: 5850\niter_count: 5860\niter_count: 5870\niter_count: 5880\niter_count: 5890\niter_count: 5900\niter_count: 5910\niter_count: 5920\niter_count: 5930\niter_count: 5940\niter_count: 5950\niter_count: 5960\niter_count: 5970\niter_count: 5980\niter_count: 5990\niter_count: 6000\niter_count: 6010\niter_count: 6020\niter_count: 6030\niter_count: 6040\niter_count: 6050\niter_count: 6060\niter_count: 6070\niter_count: 6080\niter_count: 6090\niter_count: 6100\niter_count: 6110\niter_count: 6120\niter_count: 6130\niter_count: 6140\niter_count: 6150\niter_count: 6160\niter_count: 6170\niter_count: 6180\niter_count: 6190\niter_count: 6200\niter_count: 6210\niter_count: 6220\niter_count: 6230\niter_count: 6240\niter_count: 6250\niter_count: 6260\niter_count: 6270\niter_count: 6280\niter_count: 6290\niter_count: 6300\niter_count: 6310\niter_count: 6320\niter_count: 6330\niter_count: 6340\niter_count: 6350\niter_count: 6360\niter_count: 6370\niter_count: 6380\niter_count: 6390\niter_count: 6400\niter_count: 6410\niter_count: 6420\niter_count: 6430\niter_count: 6440\niter_count: 6450\niter_count: 6460\niter_count: 6470\niter_count: 6480\niter_count: 6490\niter_count: 6500\niter_count: 6510\niter_count: 6520\niter_count: 6530\niter_count: 6540\niter_count: 6550\niter_count: 6560\niter_count: 6570\niter_count: 6580\niter_count: 6590\niter_count: 6600\niter_count: 6610\niter_count: 6620\niter_count: 6630\niter_count: 6640\niter_count: 6650\niter_count: 6660\niter_count: 6670\niter_count: 6680\niter_count: 6690\niter_count: 6700\niter_count: 6710\niter_count: 6720\niter_count: 6730\niter_count: 6740\niter_count: 6750\niter_count: 6760\niter_count: 6770\niter_count: 6780\niter_count: 6790\niter_count: 6800\niter_count: 6810\niter_count: 6820\niter_count: 6830\niter_count: 6840\niter_count: 6850\niter_count: 6860\niter_count: 6870\niter_count: 6880\niter_count: 6890\niter_count: 6900\niter_count: 6910\niter_count: 6920\niter_count: 6930\niter_count: 6940\niter_count: 6950\niter_count: 6960\niter_count: 6970\niter_count: 6980\niter_count: 6990\niter_count: 7000\niter_count: 7010\niter_count: 7020\niter_count: 7030\niter_count: 7040\niter_count: 7050\niter_count: 7060\niter_count: 7070\niter_count: 7080\niter_count: 7090\niter_count: 7100\niter_count: 7110\niter_count: 7120\niter_count: 7130\niter_count: 7140\niter_count: 7150\niter_count: 7160\niter_count: 7170\niter_count: 7180\niter_count: 7190\niter_count: 7200\niter_count: 7210\niter_count: 7220\nConverged with 7227 iterations\nThe weights we end up with is: [[ 0.27414747]\n [-0.08302919]\n [ 0.5742264 ]]\n\n\n\n# our implementation\nmysvm.big_plot(X_train, y_train, X_validate, y_validate, X_test, y_test, 9,3)\n\n\n\n\n\n\nSecond Take\nOur first implementation seems off, let us do it all over again from scratch."
  },
  {
    "objectID": "posts/blog-02-newton-raphson/Newton_Raphson_test.html",
    "href": "posts/blog-02-newton-raphson/Newton_Raphson_test.html",
    "title": "Compare Newton Raphson and Logistic Regression",
    "section": "",
    "text": "Test Newton Raphson and Logistic Regression\nThe source code of our implementation could be found in this directory on github along with this blog post.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\n# from mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics.pairwise import rbf_kernel\n\n\nfrom sklearn.datasets import make_blobs\nfrom Newton_Raphson_test import Newton_Raphson\n\n\n# create test data set\nX_train, y_train = make_blobs(n_samples=300, centers = 2, cluster_std= 2.5, n_features=2, random_state=42)\nprint(f\"shape of X_train is: {np.shape(X_train)}\")\nprint(f\"shape of y_train is: {np.shape(y_train)}\")\nprint(f\"X_train.shape[0] is: {X_train.shape[0]}\")\nprint(f\"y_train.shape[0] is: {y_train.shape[0]}\")\nprint(f\"X_train.shape[1] is: {X_train.shape[1]}\")\n\nshape of X_train is: (300, 2)\nshape of y_train is: (300,)\nX_train.shape[0] is: 300\ny_train.shape[0] is: 300\nX_train.shape[1] is: 2\n\n\n\n# plot test data set\nplt.figure(figsize=(6,6))\nplt.scatter(X_train[:,0],X_train[:,1], c=y_train)\n\n&lt;matplotlib.collections.PathCollection at 0x7f75715ee5d0&gt;\n\n\n\n\n\n\nmyNR = Newton_Raphson()\nmyNR.regress(y = y_train, X = X_train, max_iters=1e3, tol=1e-9, converged=False)\n\nnumber of iteration: 10\nbeta: [[ 0.88131503]\n [-0.8612759 ]\n [ 3.62857607]]\nnumber of iteration: 20\nbeta: [[ 1.05875831]\n [-1.08493095]\n [ 4.57156319]]\nnumber of iteration: 30\nbeta: [[ 1.05911467]\n [-1.08539209]\n [ 4.57354849]]\nnumber of iteration: 40\nbeta: [[ 1.05911502]\n [-1.08539254]\n [ 4.57355043]]\nConverged with 41 iterations\nThe beta we end up with is: [[ 1.05911502]\n [-1.08539254]\n [ 4.57355043]]\n\n\n\n# NR.big_plot(X_train, y_train, X_validate, y_validate, X_test, y_test, 30, 10)\n\n\nmyNR.bare_bone_plot(X_train, y_train, 6,6)\nprint(f\"the score of our Newton_Raphson implementation is: {myNR.score(X_train,y_train)}\")\n# print(NR.score(X_test,y_test))\n# print(NR.score(X_validate,y_validate))\n\nthe weight beta is: [[ 1.05911502]\n [-1.08539254]\n [ 4.57355043]]\nthe score of our Newton_Raphson implementation is: 0.9733333333333334\n\n\n\n\n\n\n\nNow, we take a look at the Newton’s Method in Logistic Regression provided to us by sklearn library.\n\nLR = LogisticRegression(solver=\"newton-cg\")\nLR.fit(X_train, y_train)\nLR.coef_\n\narray([[ 0.98557955, -1.00811845]])\n\n\n\n# LRscoreTest = LR.score(X_test, y_test)\n# print(LRscoreTest)\nLRscoreTrain= LR.score(X_train, y_train)\nprint(LRscoreTrain)\n\n0.9733333333333334\n\n\n\n# y_test= y_test.reshape(-1)\n# NR.simple_plot(LR, X_test, y_test, 5, 5)\n\n\nprint(LR.coef_)\na_0 = LR.coef_[0][0]\na_1 = LR.coef_[0][1]\nprint(LR.intercept_[0])\nplt.rcParams[\"figure.figsize\"] = (6,6)\nfig = plt.scatter(X_train[:,0], X_train[:,1], c = y_train)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nf1 = np.linspace(-7.5,7.5, 501)\np = plt.plot(f1, - (LR.intercept_/a_1) - (a_0/a_1)*f1, color = \"black\")\ntitle = plt.gca().set_title(f\"score= {round(LRscoreTrain,3)} using {LR}\")\n\n[[ 0.98557955 -1.00811845]]\n4.250774512606014\n\n\n\n\n\nIt looks like we made it! Our implementation and the one from sklearn perform in the same way."
  }
]