[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Final Project on prediction of Incorporation using replication data file on Russian Historical Factory level data from Professor Gregg, Middlebury College, Vermont\n\n\n\n\n\n\nMay 2, 2023\n\n\nPrateek and Xianzhi\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/final-blog-post/svm_test.html",
    "href": "posts/final-blog-post/svm_test.html",
    "title": "Our CSCI 0451 Final Project",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\n\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nFP = FinalProject()\ndf = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n/tmp/ipykernel_179926/4231091402.py:7: DtypeWarning: Columns (3,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n\n\ndf['form_dummy'] = np.where(df['Form']==1, 1, -1)\n\n\ndf.drop([\"Form\"], axis=1, inplace=True)\n\n\ndf = df.rename(columns={\"form_dummy\":\"Form\"})\n\n\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      id\n      PSZ\n      PSZ1900\n      FoundingYear\n      Province\n      Region\n      Industry\n      OntheSide\n      Age\n      ...\n      Wool\n      Cotton\n      MixedMaterials\n      Wood\n      Paper\n      MetalsandMachines\n      Foods\n      Chemical\n      Mineral\n      Form\n    \n  \n  \n    \n      0\n      0\n      4156\n      NaN\n      NaN\n      1860.0\n      30\n      CentralIndustrial\n      Paper\n      0.0\n      35.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      -1\n    \n    \n      1\n      1\n      52010\n      NaN\n      NaN\n      NaN\n      51\n      Previslitskii\n      Foods A\n      NaN\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      -1\n    \n    \n      2\n      2\n      30937\n      NaN\n      NaN\n      NaN\n      41\n      CentralBlacksoil\n      Foods A\n      NaN\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      -1\n    \n  \n\n3 rows × 66 columns\n\n\n\n\n\ndf = FP.balanced_one_minus_one(df)\ntrain, validate, test = FP.split_data(df)\ndf_train, X_train, y_train = FP.prepare_data(train)\ndf_validate, X_validate, y_validate= FP.prepare_data(validate)\ndf_test, X_test, y_test = FP.prepare_data(test)\n\ndf incorporated have 2393 many rows\nafter balancing, df not incorporated have 2393 many rows\n\n\n\ndf = df[['id', 'Form', 'FoundingYear',\n       'Province', 'Region', 'Industry', 'OntheSide', 'Age', 'TaxedActivity',\n       'YEAR', 'SubindustryCode', 'STCAP', 'Revenue',\n       'TotalWorkers', 'TotalPower', 'GrandTotalWorkers', 'RevperWorker',\n       'PowerperWorker', 'RevperGrandWorker', 'PowerperGrandWorker',\n       'logRevperWorker', 'logPowerperWorker', 'logRevperGrandWorker',\n       'logPowerperGrandWorker', 'logRev', 'logWorkers', 'logPower',\n       'RegIndGroup', 'RegIndYearGroup', 'ProvIndGroup', 'ProvIndYearGroup',\n       'IndYearGroup', 'IndustryFactor', 'ProvinceFactor', 'YearFactor',\n       'AKTS', 'PAI', 'factory_id', 'FormNextYear', 'FormNextNextYear',\n       'FactoryisCorpin1894', 'FormNextYearin1894', 'FactoryisCorpin1900',\n       'FormNextYearin1900', 'FactoryisCorpin1908', 'NEWDEV', 'SHARES',\n       'STPRICE', 'BONDS', 'Silk', 'Flax', 'Animal', 'Wool', 'Cotton',\n       'MixedMaterials', 'Wood', 'Paper', 'MetalsandMachines', 'Foods',\n       'Chemical', 'Mineral']]\n\n\ncols = ['PowerperWorker', 'RevperGrandWorker']\ncols = ['logWorkers', 'logPower'] \nNR = Newton_Raphson()\nFP = FinalProject()\nX_train,y_train = FP.make_ready_for_regression(X_train, y_train, cols) \nX_validate,y_validate= FP.make_ready_for_regression(X_validate, y_validate, cols) \nX_test, y_test = FP.make_ready_for_regression(X_test, y_test, cols)\n\n\nmySVC = SVC(kernel=\"linear\", gamma=\"auto\", shrinking=False)\nmySVC.fit(X_test, y_test)\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\nfrom newton_raphson import Newton_Raphson \nNR2 = Newton_Raphson()\ny_train = y_train.reshape(-1)\ny_test = y_test.reshape(-1)\nNR2.simple_plot(model = mySVC, X = X_test, y = y_test, size_1=5, size_2=5)\nprint(\"SVC\")\n\nSVC\n\n\n\n\n\n\nprint(mySVC.coef_)\nprint(mySVC.intercept_)\n\n[[0.83995613 0.08197002]]\n[-3.79364818]\n\n\n\nrow_length = X_train.shape[1]+1\nprint(row_length)\n\n3\n\n\n\nmysvm = supp_vec_machine(row_length=row_length)\nmysvm.fit(X_test,y_test, max_iter=1e3, alpha=1, tol=1e-5, lamb=0.5)\n\niter_count: 10\niter_count: 20\niter_count: 30\niter_count: 40\niter_count: 50\niter_count: 60\niter_count: 70\niter_count: 80\niter_count: 90\niter_count: 100\niter_count: 110\niter_count: 120\niter_count: 130\niter_count: 140\niter_count: 150\niter_count: 160\niter_count: 170\niter_count: 180\niter_count: 190\niter_count: 200\niter_count: 210\niter_count: 220\niter_count: 230\niter_count: 240\niter_count: 250\niter_count: 260\niter_count: 270\niter_count: 280\niter_count: 290\niter_count: 300\niter_count: 310\niter_count: 320\niter_count: 330\niter_count: 340\niter_count: 350\niter_count: 360\niter_count: 370\niter_count: 380\niter_count: 390\nConverged with 400 iterations\nThe weights we end up with is: [[ 0.18285496]\n [ 0.03940883]\n [-0.33444781]]\n\n\n\nprint(mysvm.weights)\nprint(mysvm.beta)\n\n[[ 0.18285496]\n [ 0.03940883]\n [-0.33444781]]\n[[ 0.18285496]\n [ 0.03940883]\n [-0.33444781]]\n\n\n\nmysvm.big_plot(X_train, y_train, X_validate, y_validate, X_test, y_test, 9,3)\n\n\n\n\n\nfrom sklearn.datasets import make_moons\n\nX,y= make_moons(100, shuffle=True, noise = 0.2)\nplt.rcParams[\"figure.figsize\"] = (4,4)\nplt.scatter(X[:,0], X[:,1], c=y)\nlabels=plt.gca().set(xlabel=\"Feature 1\", ylabel=\"Feature 2\")\n\n\n\n\n\nmysvm1 = supp_vec_machine()\nmysvm1.fit(X,y, max_iter=1e3, alpha=1, tol=1e-6, lamb=0.5)\n\niter_count: 10\niter_count: 20\niter_count: 30\niter_count: 40\niter_count: 50\niter_count: 60\niter_count: 70\niter_count: 80\niter_count: 90\niter_count: 100\niter_count: 110\niter_count: 120\niter_count: 130\niter_count: 140\niter_count: 150\niter_count: 160\niter_count: 170\niter_count: 180\niter_count: 190\niter_count: 200\niter_count: 210\niter_count: 220\niter_count: 230\niter_count: 240\niter_count: 250\niter_count: 260\niter_count: 270\niter_count: 280\niter_count: 290\niter_count: 300\niter_count: 310\niter_count: 320\niter_count: 330\niter_count: 340\niter_count: 350\niter_count: 360\niter_count: 370\niter_count: 380\niter_count: 390\niter_count: 400\niter_count: 410\niter_count: 420\niter_count: 430\niter_count: 440\niter_count: 450\niter_count: 460\niter_count: 470\niter_count: 480\niter_count: 490\niter_count: 500\niter_count: 510\niter_count: 520\niter_count: 530\niter_count: 540\niter_count: 550\niter_count: 560\niter_count: 570\niter_count: 580\niter_count: 590\niter_count: 600\niter_count: 610\niter_count: 620\niter_count: 630\niter_count: 640\niter_count: 650\niter_count: 660\niter_count: 670\niter_count: 680\niter_count: 690\niter_count: 700\niter_count: 710\niter_count: 720\niter_count: 730\niter_count: 740\niter_count: 750\niter_count: 760\niter_count: 770\niter_count: 780\niter_count: 790\niter_count: 800\niter_count: 810\niter_count: 820\niter_count: 830\niter_count: 840\niter_count: 850\niter_count: 860\niter_count: 870\niter_count: 880\niter_count: 890\niter_count: 900\niter_count: 910\niter_count: 920\niter_count: 930\niter_count: 940\niter_count: 950\niter_count: 960\niter_count: 970\niter_count: 980\niter_count: 990\n\n\n\nyourSVC = SVC(kernel=\"linear\", gamma=\"auto\", shrinking=False)\nyourSVC.fit(X, y)\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\nprint(yourSVC.coef_)\nprint(yourSVC.intercept_)\n\n[[ 0.62099012 -1.89131465]]\n[0.16174384]"
  },
  {
    "objectID": "posts/final-blog-post/index.html",
    "href": "posts/final-blog-post/index.html",
    "title": "ML Final Project",
    "section": "",
    "text": "The source code for this project could be found here. We have a README.md file that outlines what our project want to achieve, and roughly how we are going to implement the models and analysis. We also have a separate .txt file that gives a dictionary of all the variable names and their actual meaning. Hence, we encourage the reader to also reference that .txt file to remember which variable means what."
  },
  {
    "objectID": "posts/final-blog-post/index.html#data",
    "href": "posts/final-blog-post/index.html#data",
    "title": "ML Final Project",
    "section": "Data",
    "text": "Data\n\nWhere the data comes from\n\nThe Russian factory censuses from 1894, 1900 and 1908\n\nWhat the data includes\n\nThe data is factory level and includes the following information:\n\nWhat we’re hoping to do with the data\nSome summary statistics\n\nFirst, let us import some libraries that will become useful down the road. Also, the following snippet will automatically reload the final_project_code.py file where we keep our functions.\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom final_project_code import FinalProject \n\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\n\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nRead in the data\nLet us create an instance of the class we defined in final_project_code.py, and let us read in the entire data set.\n\n# from sklearn.metrics.pairwise import rbf_kernel\nFP = FinalProject()\n\n\nRvss = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n/tmp/ipykernel_182052/139095848.py:1: DtypeWarning: Columns (3,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  Rvss = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n\nFirst, let’s do some data visualization. We are interested in seeing which industry in Late Imperial Russia had high machine power (measured in horsepower) and have high number of workers. We also want to get a sense of the distribution of machine power and number of workers, and visualize them by industry. Hence, let’s focus on the picture below. We see that roughly, factory with more machine power tend to also have more workers, and most company cluster at the \\(2000\\) horse power level, and \\(2500\\) workers.\n\nsns.jointplot(data=Rvss, x=\"TotalWorkers\", y = \"TotalPower\", hue=\"Industry\")\n\n<seaborn.axisgrid.JointGrid at 0x7fe856046b80>\n\n\n\n\n\nSimilarly, here’s another plot to visualize the unbalanced nature of the data set. Here, Form is the desired label that we want to predict. Form taking a value of \\(1\\) means that factory was incorporated, i.e., it was owned by a incorporated firm. If Form take the value of \\(0\\), then that factory was not incorporated. In the next plot, instead of TotalPower, which stands for Total amount of horse power and TotalWorkers, which stands for total number of workers, we use logPowerperWorker and logRevperWorker as our y-axis and our x-axis. logPowerperWorker is obtained by taking the log of \\(\\frac{Power}{Worker}\\), and logRevperWorker is log of \\(\\frac{Revenue}{Worker}\\). And the hue is whether the factory is encorporated or not. Again, we see that the orange dots, which corresponds to \\(1\\), which corresponds to encorporated, is a very small percentage of all the factories. Most factories are not encorporated. Also, we observe that the data points follows a bell-shaped distribution on the two dimensions.\n\nsns.jointplot(data=Rvss, x=\"logRevperWorker\", y=\"logPowerperWorker\", hue = \"Form\")\n\n<seaborn.axisgrid.JointGrid at 0x7fe8553b1b80>\n\n\n\n\n\n\n\nPredicting whether Russian Factories want to incorporate or not\n\n\nFirst approach, rebalance the data set by random sampling\nWe start our analysis by using logistic regression to predict what kind of firms in late Imperial Russia is more likely owned by a corporation. Since our data is not balanced, we use several different approach to this problem and try them one at a time. Luckily for us, classification problem with unbalanced data labels is quite common, so we have many approaches at our disposal.\nLet us first get out definitions straight. Unbalanced data refers to those datasets where the target label has an uneven distribution of observations, First, we try to randomly sample the majority data set, which in this case, is when the label equals unincorporated. Then we keep all the data entries of the minority data set, and add in the randomly sampled extract of the majority data set with size equal to the minority data set. Then we perform logistic regression on this new data set. The good news is that our new data set is balanced, and the bad news is that we loose a lot of information by discarding many data entries in the majority data set.\nWhether to incorporate or not is an interesting question for factories and firms in late Imperial Russia. There were many factors that might affect a firm’s decision to incorporate or not, including the overall size of the factory, which could be seen in features such as total machine power of the factory and the total number of workers in a factory. Other factors such as the geographical location of the factory (i.e. which region it was located in) could also play a role. Since the decision to incorporate could be affected by many features, we see that some feature engineering could be beneficial for our analysis.\nWe download the replication data set and put it in the same directory as our project. After we read in the data, we notice that there are \\(66\\) columns, which means potentially we could have around \\(60\\) features for our machine learning model. However, let’s start small. Hence, we begin our analysis using a subset of the columns. Also, since in the original data set, there’s only a small percentage of factories that are incorporated, which is because of historical reasons in Late Imperial Russia during 1894 to 1908. For the purpose of this machine learning project, we artificially select a subset of the whole data set so that we have equal number of factories owned by incorporated firms and not incorporated firms alike.\n\nRvss.loc[Rvss['Form']==0]\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      id\n      Form\n      PSZ\n      PSZ1900\n      FoundingYear\n      Province\n      Region\n      Industry\n      OntheSide\n      ...\n      Animal\n      Wool\n      Cotton\n      MixedMaterials\n      Wood\n      Paper\n      MetalsandMachines\n      Foods\n      Chemical\n      Mineral\n    \n  \n  \n    \n      0\n      0\n      4156\n      0\n      NaN\n      NaN\n      1860.0\n      30\n      CentralIndustrial\n      Paper\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      1\n      52010\n      0\n      NaN\n      NaN\n      NaN\n      51\n      Previslitskii\n      Foods A\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      2\n      2\n      30937\n      0\n      NaN\n      NaN\n      NaN\n      41\n      CentralBlacksoil\n      Foods A\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      3\n      3\n      39923\n      0\n      NaN\n      NaN\n      NaN\n      3\n      Caucasus\n      Metals and Machines\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      4\n      3296\n      0\n      NaN\n      NaN\n      1882.0\n      11\n      Northwestern\n      Chemicals\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      37964\n      37964\n      34420\n      0\n      NaN\n      NaN\n      NaN\n      30\n      CentralIndustrial\n      Mineral Products\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      37965\n      37965\n      34210\n      0\n      NaN\n      NaN\n      NaN\n      19\n      CentralIndustrial\n      Mineral Products\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      37966\n      37966\n      52603\n      0\n      NaN\n      NaN\n      NaN\n      17\n      Southern\n      Foods A\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      37967\n      37967\n      4886\n      0\n      NaN\n      NaN\n      NaN\n      61\n      CentralIndustrial\n      Mineral Products\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      37968\n      37968\n      37189\n      0\n      NaN\n      NaN\n      NaN\n      8\n      Caucasus\n      Silk\n      NaN\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n37895 rows × 66 columns\n\n\n\n\ndf = Rvss[['id', 'Form', 'FoundingYear',\n       'Province', 'Region', 'Industry', 'OntheSide', 'Age', 'TaxedActivity',\n       'YEAR', 'SubindustryCode', 'STCAP', 'Revenue',\n       'TotalWorkers', 'TotalPower', 'GrandTotalWorkers', 'RevperWorker',\n       'PowerperWorker', 'RevperGrandWorker', 'PowerperGrandWorker',\n       'logRevperWorker', 'logPowerperWorker', 'logRevperGrandWorker',\n       'logPowerperGrandWorker', 'logRev', 'logWorkers', 'logPower',\n       'RegIndGroup', 'RegIndYearGroup', 'ProvIndGroup', 'ProvIndYearGroup',\n       'IndYearGroup', 'IndustryFactor', 'ProvinceFactor', 'YearFactor',\n       'AKTS', 'PAI', 'factory_id', 'FormNextYear', 'FormNextNextYear',\n       'FactoryisCorpin1894', 'FormNextYearin1894', 'FactoryisCorpin1900',\n       'FormNextYearin1900', 'FactoryisCorpin1908', 'NEWDEV', 'SHARES',\n       'STPRICE', 'BONDS', 'Silk', 'Flax', 'Animal', 'Wool', 'Cotton',\n       'MixedMaterials', 'Wood', 'Paper', 'MetalsandMachines', 'Foods',\n       'Chemical', 'Mineral']]\n\n\nresult = FP.create_balanced_data(df)\n\ndf incorporated have 2393 many rows\nafter balancing, df not incorporated have 2393 many rows\n\n\n\nresult['Form'].mean()\n\n0.5\n\n\n\n# compression_opts = dict(method='zip', archive_name='artificially_balanced_Rvssian_Factory.csv')\n# result.to_csv('artificially_balanced_Rvssian_Factory.zip', index = False, compression=compression_opts)\n\n\nFP = FinalProject()\ntrain, validate, test = FP.split_data(result)\n\n\ndf_train, X_train, y_train = FP.prepare_data(train)\ndf_validate, X_validate, y_validate= FP.prepare_data(validate)\ndf_test, X_test, y_test = FP.prepare_data(test)"
  },
  {
    "objectID": "posts/final-blog-post/test.html",
    "href": "posts/final-blog-post/test.html",
    "title": "Our CSCI 0451 Final Project",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nfrom final_project_code import FinalProject \nfrom newton_raphson import Newton_Raphson\nfrom final_plot import plot_stuff\n\n\n# from sklearn.metrics.pairwise import rbf_kernel\nFP = FinalProject()\ndf = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n/tmp/ipykernel_13695/3545232409.py:3: DtypeWarning: Columns (3,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"./AG_Corp_Prod_DataBase.csv\")\n\n\n\ndf = FP.create_balanced_data(df)\n# df = df.sample(n=10000, replace=False)\ntrain, validate, test = FP.split_data(df)\n\ndf incorporated have 2393 many rows\nafter balancing, df not incorporated have 2393 many rows\n\n\n\ncols=['TotalWorkers','TotalPower']\n# RegionCoded is not generated here\ncols = [\"RegionCoded\", \"FactoryisCorpin1900\", \"NEWDEV\"]\ncols = cols[1:]\n\n\ndf_train, X_train, y_train = FP.prepare_data(train)\ndf_validate, X_validate, y_validate= FP.prepare_data(validate)\ndf_test, X_test, y_test = FP.prepare_data(test)\n\n\ncols = ['PowerperWorker', 'RevperGrandWorker']\ncols = ['logWorkers', 'logPower'] \n\n\n# only run once, convert to numpy\nX_train, y_train = FP.make_ready_for_regression(X_train, y_train, cols)\nX_validate, y_validate= FP.make_ready_for_regression(X_validate, y_validate, cols)\nX_test, y_test= FP.make_ready_for_regression(X_test, y_test, cols)\n\n\nfrom newton_raphson import Newton_Raphson\nNR = Newton_Raphson() \nNR.regress(y = y_train, X = X_train, max_iters = 1e3, tol=1e-15, converged=False)\n\nlearning rate is: 0.5\nRegularization is: True\nnumber of iteration: 10\nbeta: [[  516.28805794]\n [ 1787.67195079]\n [-1429.11939363]]\n\n\n/home/xianzhiwang/ml0451/ml-0451-final-proj/posts/final-blog-post/newton_raphson.py:35: RuntimeWarning: overflow encountered in exp\n  return 1/(1+np.exp(-x))\n\n\nnumber of iteration: 20\nbeta: [[  263.27490383]\n [   96.02404839]\n [-1355.20349548]]\nnumber of iteration: 30\nbeta: [[ 179.27917401]\n [   7.14642413]\n [-787.34272352]]\nnumber of iteration: 40\nbeta: [[  54.02617268]\n [   2.11965938]\n [-237.16886663]]\nnumber of iteration: 50\nbeta: [[ 0.73015557]\n [ 0.11186911]\n [-3.29529669]]\nnumber of iteration: 60\nbeta: [[ 1.06334261]\n [ 0.10968743]\n [-4.65452511]]\nnumber of iteration: 70\nbeta: [[ 1.06405021]\n [ 0.10969094]\n [-4.65747183]]\nnumber of iteration: 80\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nnumber of iteration: 90\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nnumber of iteration: 100\nbeta: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\nConverged with 104 iterations\nThe beta we end up with is: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\n\n\n\nNR.big_plot(X_train, y_train, X_validate, y_validate, X_test, y_test, 30, 10)\n\n\n\n\n\nNR.bare_bone_plot(X_train, y_train, 5,5)\nprint(NR.score(X_train,y_train))\nprint(NR.score(X_test,y_test))\nprint(NR.score(X_validate,y_validate))\n\nthe weight beta is: [[ 1.06405114]\n [ 0.10969094]\n [-4.65747577]]\n0.8025078369905956\n0.8173277661795407\n0.7899686520376176\n\n\n\n\n\n\n# from final_plot import plot_stuff\n# PS = plot_stuff()\ncols = ['logWorkers', 'logPower'] \n\n\nLR = LogisticRegression(solver=\"newton-cg\")\nLR.fit(X_train, y_train)\nLR.coef_\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\narray([[1.06218722, 0.10987587]])\n\n\n\nLRscoreTest = LR.score(X_test, y_test)\nprint(LRscoreTest)\nLRscoreTrain= LR.score(X_train, y_train)\nprint(LRscoreTrain)\n\n0.8173277661795407\n0.8025078369905956\n\n\n\ny_test= y_test.reshape(-1)\nNR.simple_plot(LR, X_test, y_test, 5, 5)\n\n\n\n\n\nprint(LR.coef_)\na_0 = LR.coef_[0][0]\na_1 = LR.coef_[0][1]\nprint(LR.intercept_[0])\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfig = plt.scatter(X_train[:,0], X_train[:,1], c = y_train)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nf1 = np.linspace(3.5,4.5, 501)\np = plt.plot(f1, - (LR.intercept_/a_1) - (a_0/a_1)*f1, color = \"black\")\ntitle = plt.gca().set_title(f\"score= {round(LRscoreTrain,3)} using {LR}\")\n\n[[1.06218722 0.10987587]]\n-4.650112405682134\n\n\n\n\n\n\nfrom newton_raphson import Newton_Raphson\nNR2 = Newton_Raphson()\n\n\nDTree = DecisionTreeClassifier(random_state=0)\nDTree.fit(X_train, y_train)\nDTree.score(X_train, y_train)\n\n0.9191919191919192\n\n\n\ny_train = y_train.reshape(-1)\nNR2.simple_plot(DTree, X_train, y_train, 4,4)\n\n\n\n\n\nRandomForest = RandomForestClassifier()\nRandomForest.fit(X_train, y_train)\nRandomForest.score(X_train, y_train)\n\n0.9191919191919192\n\n\n\nNR2.simple_plot(RandomForest, X_train, y_train, 4,4)\n\n\n\n\n\nSVC2 = SVC(kernel=\"linear\", gamma = \"auto\", shrinking=False)\n\n\nSVC2.fit(X_train, y_train)\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\nNR2.simple_plot(SVC2, X_train, y_train,5,5)"
  }
]